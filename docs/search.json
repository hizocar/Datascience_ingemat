[
  {
    "objectID": "data_science_ingemat/presentacion.html",
    "href": "data_science_ingemat/presentacion.html",
    "title": "Presentando un proyecto",
    "section": "",
    "text": "Objetivo del Proyecto: Comienza con una visión general del proyecto y sus objetivos. Explica por qué se realizó el proyecto y cuál es su relevancia para la empresa.\n\n\n\nProblema a Resolver: Describe el problema que tu proyecto de ciencia de datos está tratando de resolver. Haz esto de una manera que sea comprensible para personas que no son expertas en el tema, utilizando ejemplos y analogías simples cuando sea posible.\n\n\n\nRecopilación de Datos: Explica cómo y de dónde obtuviste los datos para el proyecto.\n\n\nDescribe cómo se prepararon los datos para el análisis. Puedes mencionar técnicas de limpieza de datos, pero evita entrar en demasiados detalles técnicos.\n\n\n\nDescribe las técnicas y algoritmos que utilizaste para desarrollar el modelo de ciencia de datos. Explica por qué elegiste esos enfoques y cómo se relacionan con el problema que estás tratando de resolver. Intenta mantener la explicación accesible y evita el uso de jerga técnica tanto como sea posible.\n\n\n\n\nInterpretación de Resultados: Presenta los resultados de tu análisis de una manera fácil de entender. Utiliza gráficos y tablas para visualizar los datos y destacar los hallazgos clave. Evita presentar demasiados números crudos; en su lugar, trata de resumir y explicar los resultados de una manera que sea fácil de entender.\n\n\nExplica lo que los resultados significan para el negocio. ¿Qué acciones debería tomar la empresa basándose en estos resultados? ¿Cómo ayudará esto a la empresa a alcanzar sus objetivos?\n\n\n\n\n\n\nResume los hallazgos clave de tu proyecto y cómo estos hallazgos pueden beneficiar a la empresa.\n\n\n\nProporciona recomendaciones para los próximos pasos. ¿Hay otras preguntas que podrían explorarse en el futuro? ¿Cómo puede la empresa implementar los resultados de tu proyecto?\n\n\n\n\nDeja tiempo para preguntas al final de la presentación. Esto da a la audiencia la oportunidad de aclarar cualquier aspecto que no entienda y permite que se realicen preguntas más profundas que pueden no ser adecuadas durante la presentación principal.\nRecuerda, la clave es mantener el lenguaje simple y accesible, y tratar de relacionar todo con los objetivos y necesidades de la empresa. Tu meta es hacer que la audiencia comprenda el valor de tu trabajo, no impresionarlos con jerga técnica."
  },
  {
    "objectID": "data_science_ingemat/presentacion.html#cómo-presentar-un-proyecto-de-ciencias-de-datos",
    "href": "data_science_ingemat/presentacion.html#cómo-presentar-un-proyecto-de-ciencias-de-datos",
    "title": "Presentando un proyecto",
    "section": "",
    "text": "Objetivo del Proyecto: Comienza con una visión general del proyecto y sus objetivos. Explica por qué se realizó el proyecto y cuál es su relevancia para la empresa.\n\n\n\nProblema a Resolver: Describe el problema que tu proyecto de ciencia de datos está tratando de resolver. Haz esto de una manera que sea comprensible para personas que no son expertas en el tema, utilizando ejemplos y analogías simples cuando sea posible.\n\n\n\nRecopilación de Datos: Explica cómo y de dónde obtuviste los datos para el proyecto.\n\n\nDescribe cómo se prepararon los datos para el análisis. Puedes mencionar técnicas de limpieza de datos, pero evita entrar en demasiados detalles técnicos.\n\n\n\nDescribe las técnicas y algoritmos que utilizaste para desarrollar el modelo de ciencia de datos. Explica por qué elegiste esos enfoques y cómo se relacionan con el problema que estás tratando de resolver. Intenta mantener la explicación accesible y evita el uso de jerga técnica tanto como sea posible.\n\n\n\n\nInterpretación de Resultados: Presenta los resultados de tu análisis de una manera fácil de entender. Utiliza gráficos y tablas para visualizar los datos y destacar los hallazgos clave. Evita presentar demasiados números crudos; en su lugar, trata de resumir y explicar los resultados de una manera que sea fácil de entender.\n\n\nExplica lo que los resultados significan para el negocio. ¿Qué acciones debería tomar la empresa basándose en estos resultados? ¿Cómo ayudará esto a la empresa a alcanzar sus objetivos?\n\n\n\n\n\n\nResume los hallazgos clave de tu proyecto y cómo estos hallazgos pueden beneficiar a la empresa.\n\n\n\nProporciona recomendaciones para los próximos pasos. ¿Hay otras preguntas que podrían explorarse en el futuro? ¿Cómo puede la empresa implementar los resultados de tu proyecto?\n\n\n\n\nDeja tiempo para preguntas al final de la presentación. Esto da a la audiencia la oportunidad de aclarar cualquier aspecto que no entienda y permite que se realicen preguntas más profundas que pueden no ser adecuadas durante la presentación principal.\nRecuerda, la clave es mantener el lenguaje simple y accesible, y tratar de relacionar todo con los objetivos y necesidades de la empresa. Tu meta es hacer que la audiencia comprenda el valor de tu trabajo, no impresionarlos con jerga técnica."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "",
    "text": "ARIMA es un acrónimo que significa AutoRegressive Integrated Moving Average. Es una clase de modelos que explica una serie de tiempo dada en función de sus propios valores pasados, es decir, su propia lags. Puede ser percibido como una regresión lineal de los datos pasados.\nLos componentes de los modelos ARIMA son:\n\nAR: Autoregression. Un modelo que utiliza la relación dependiente entre una observación y un número de observaciones rezagadas (lagged observations).\nI: Integrated. El uso de la diferenciación de las observaciones en bruto (por ejemplo, restar una observación de una observación en el paso de tiempo anterior) para hacer que la serie de tiempo sea estacionaria.\nMA: Moving Average. Un modelo que utiliza la dependencia entre una observación y un error residual de un modelo de media móvil aplicado a observaciones rezagadas.\n\nUn modelo ARIMA se denota como ARIMA(p,d,q) donde:\n\np es el número de términos autorregresivos (AR part). Permite incorporar el efecto de los valores pasados en nuestro modelo.\nd es el número de diferencias no estacionales necesarias para la estacionariedad.\nq es el número de errores de pronóstico rezagados en la ecuación de predicción (MA part)."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-arima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-arima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "",
    "text": "ARIMA es un acrónimo que significa AutoRegressive Integrated Moving Average. Es una clase de modelos que explica una serie de tiempo dada en función de sus propios valores pasados, es decir, su propia lags. Puede ser percibido como una regresión lineal de los datos pasados.\nLos componentes de los modelos ARIMA son:\n\nAR: Autoregression. Un modelo que utiliza la relación dependiente entre una observación y un número de observaciones rezagadas (lagged observations).\nI: Integrated. El uso de la diferenciación de las observaciones en bruto (por ejemplo, restar una observación de una observación en el paso de tiempo anterior) para hacer que la serie de tiempo sea estacionaria.\nMA: Moving Average. Un modelo que utiliza la dependencia entre una observación y un error residual de un modelo de media móvil aplicado a observaciones rezagadas.\n\nUn modelo ARIMA se denota como ARIMA(p,d,q) donde:\n\np es el número de términos autorregresivos (AR part). Permite incorporar el efecto de los valores pasados en nuestro modelo.\nd es el número de diferencias no estacionales necesarias para la estacionariedad.\nq es el número de errores de pronóstico rezagados en la ecuación de predicción (MA part)."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-sarima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#modelos-sarima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Modelos SARIMA",
    "text": "Modelos SARIMA\nLos modelos SARIMA (Seasonal Autoregressive Integrated Moving Average) son una extensión de los modelos ARIMA que soportan directamente las series de tiempo univariadas con un componente estacional.\nUn modelo SARIMA se denota como SARIMA(p,d,q)(P,D,Q)m donde:\n\np, d, q son los parámetros del modelo ARIMA para las tendencias no estacionales.\nP, D, Q son los parámetros del modelo ARIMA para las tendencias estacionales.\nm se refiere al número de periodos en cada temporada."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#ejemplo-en-python",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#ejemplo-en-python",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Ejemplo en Python",
    "text": "Ejemplo en Python\nAquí hay un ejemplo de cómo ajustar un modelo ARIMA y SARIMA a una serie de tiempo en Python usando la biblioteca statsmodels.\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Cargar datos\nseries = pd.read_csv('your_data.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n\n# Ajustar modelo ARIMA\nmodel_arima = ARIMA(series, order=(5,1,0))\nmodel_arima_fit = model_arima.fit(disp=0)\nprint(model_arima_fit.summary())\n\n# Ajustar modelo SARIMA\nmodel_sarima = SARIMAX(series, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nmodel_sarima_fit = model_sarima.fit(disp=False)\nprint(model_sarima_fit.summary())\nPor favor, reemplace 'your_data.csv' con la ruta a su archivo de datos. Los parámetros (5,1,0) y (1, 1, 1, 12) son solo ejemplos y deben ser ajustados a sus datos específicos."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-arima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-arima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Detalles adicionales sobre los modelos ARIMA",
    "text": "Detalles adicionales sobre los modelos ARIMA\nLos modelos ARIMA son aplicados en algunos casos donde los datos muestran evidencia de no estacionariedad, donde una etapa inicial de diferenciación (correspondiente al componente ‘I’ en el modelo) puede ser aplicada una o más veces para eliminar la tendencia no estacionaria.\nEl componente AR del modelo ARIMA indica que la variable evolutiva de interés es regresada en sus propios valores rezagados, es decir, anteriores. Por ejemplo, si p es 5, los predictores para x(t) serán x(t-1)….x(t-5).\nEl componente MA del modelo ARIMA indica que el error de regresión es en realidad una combinación lineal de términos de error cuyos valores ocurrieron contemporáneamente y en varios momentos en el pasado.\nEl componente I del modelo ARIMA indica que los datos se han diferenciado al menos una vez para hacer la serie estacionaria."
  },
  {
    "objectID": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-sarima",
    "href": "data_science_ingemat/modulo2/ARIMA_SARIMA_Tutorial.html#detalles-adicionales-sobre-los-modelos-sarima",
    "title": "Quiz: Modelos ARIMA y SARIMA",
    "section": "Detalles adicionales sobre los modelos SARIMA",
    "text": "Detalles adicionales sobre los modelos SARIMA\nLos modelos SARIMA añaden una capa adicional de complejidad al modelado de series temporales. Estos modelos incorporan elementos de estacionalidad tanto en el componente AR como en el MA del modelo. Esto significa que el modelo no sólo considera las relaciones de los datos consigo mismo (AR), los errores (MA) y las diferencias para la estacionariedad (I), sino que también tiene en cuenta la estacionalidad de los datos.\nEl componente SAR del modelo SARIMA indica que la serie temporal tiene correlaciones estacionales autorregresivas. Por ejemplo, si P es 2, los predictores para x(t) serán x(t-12) y x(t-24) si la serie es mensual.\nEl componente SMA del modelo SARIMA indica que la serie temporal tiene correlaciones estacionales de media móvil. Por ejemplo, si Q es 1, los predictores para x(t) serán los errores en los periodos t-12 y t-24 si la serie es mensual.\nEl componente SI del modelo SARIMA indica que los datos se han diferenciado al menos una vez para eliminar la estacionalidad.\nEl parámetro m en el modelo SARIMA se refiere al número de periodos por temporada, y es fundamental para modelar correctamente la estacionalidad. Por ejemplo, m sería 4 para datos trimestrales, 12 para datos mensuales, o 7 para datos diarios si se espera una estacionalidad semanal.\n\nQUIZ:\nA continuación responde el siguiente quiz: https://forms.gle/aA9Mx1D5BNvhb1SL7"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html",
    "href": "data_science_ingemat/modulo2/complemento.html",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos ARIMA y SARIMA son una extensión de los modelos de regresión lineal que se utilizan para modelar series de tiempo no estacionarias. Estos modelos pueden capturar una amplia gama de estructuras de dependencia temporal.\n\nLectura recomendada: ARIMA model as a tool in forecasting the variables: A case study\n\n\n\n\nLos modelos de volatilidad estocástica son útiles para modelar series de tiempo financieras, donde la volatilidad (la varianza de los errores) puede cambiar con el tiempo.\n\nLectura recomendada: Stochastic Volatility Modeling\n\n\n\n\nEstos modelos permiten que los parámetros de un modelo de regresión cambien en diferentes “regímenes” o estados del mundo.\n\nLectura recomendada: A new approach to the economic analysis of nonstationary time series and the business cycle\n\n\n\n\nLos modelos de espacio de estados son una clase muy general de modelos de series de tiempo que incluyen modelos ARIMA y muchos otros como casos especiales. El filtro de Kalman es un algoritmo que se utiliza para estimar las variables latentes en estos modelos.\n\nLectura recomendada: Time Series Analysis by State Space Methods\n\n\n\n\nEstos modelos permiten analizar varias series de tiempo a la vez, capturando las interdependencias entre las series.\n\nLectura recomendada: Applied Multivariate Time Series Analysis: Foundations and Trends in Econometrics\n\n\n\n\nEl aprendizaje automático ofrece una serie de técnicas que pueden ser útiles para modelar series de tiempo, especialmente cuando se dispone de grandes cantidades de datos.\n\nLectura recomendada: Deep Learning for Time Series Forecasting"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-arima-y-sarima",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-arima-y-sarima",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos ARIMA y SARIMA son una extensión de los modelos de regresión lineal que se utilizan para modelar series de tiempo no estacionarias. Estos modelos pueden capturar una amplia gama de estructuras de dependencia temporal.\n\nLectura recomendada: ARIMA model as a tool in forecasting the variables: A case study"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-volatilidad-estocástica",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-volatilidad-estocástica",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos de volatilidad estocástica son útiles para modelar series de tiempo financieras, donde la volatilidad (la varianza de los errores) puede cambiar con el tiempo.\n\nLectura recomendada: Stochastic Volatility Modeling"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-regresión-con-cambios-de-régimen",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-regresión-con-cambios-de-régimen",
    "title": "Temas complementarios",
    "section": "",
    "text": "Estos modelos permiten que los parámetros de un modelo de regresión cambien en diferentes “regímenes” o estados del mundo.\n\nLectura recomendada: A new approach to the economic analysis of nonstationary time series and the business cycle"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-espacio-de-estados-y-filtros-de-kalman",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-espacio-de-estados-y-filtros-de-kalman",
    "title": "Temas complementarios",
    "section": "",
    "text": "Los modelos de espacio de estados son una clase muy general de modelos de series de tiempo que incluyen modelos ARIMA y muchos otros como casos especiales. El filtro de Kalman es un algoritmo que se utiliza para estimar las variables latentes en estos modelos.\n\nLectura recomendada: Time Series Analysis by State Space Methods"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-multivariadas",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-multivariadas",
    "title": "Temas complementarios",
    "section": "",
    "text": "Estos modelos permiten analizar varias series de tiempo a la vez, capturando las interdependencias entre las series.\n\nLectura recomendada: Applied Multivariate Time Series Analysis: Foundations and Trends in Econometrics"
  },
  {
    "objectID": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-con-machine-learning",
    "href": "data_science_ingemat/modulo2/complemento.html#modelos-de-series-de-tiempo-con-machine-learning",
    "title": "Temas complementarios",
    "section": "",
    "text": "El aprendizaje automático ofrece una serie de técnicas que pueden ser útiles para modelar series de tiempo, especialmente cuando se dispone de grandes cantidades de datos.\n\nLectura recomendada: Deep Learning for Time Series Forecasting"
  },
  {
    "objectID": "data_science_ingemat/modulo2/introduccion.html",
    "href": "data_science_ingemat/modulo2/introduccion.html",
    "title": "Tema 1: Introducción",
    "section": "",
    "text": "Introducción a las Series de Tiempo\nUna serie de tiempo es una secuencia de observaciones tomadas secuencialmente en el tiempo. Es una herramienta esencial en el análisis de datos temporales y se utiliza en muchos campos, como la economía, las ciencias sociales, la física, y la ingeniería.\n\nDefinición Formal\nFormalmente, una serie de tiempo puede ser vista como un conjunto de puntos de datos indexados (o listados o graficados) en el orden del tiempo. Generalmente, una serie de tiempo se representa como:\nX = {X1, X2,..., Xt,..., Xn}\ndonde Xt representa la observación en el tiempo t.\n\n\nAplicaciones de las Series de Tiempo\nLas series de tiempo se utilizan en una amplia variedad de aplicaciones, incluyendo:\n\nEconomía: para el análisis de indicadores económicos, como el producto interno bruto, la inflación, las tasas de desempleo, etc.\nFinanzas: para el seguimiento y la predicción de los precios de las acciones, las tasas de interés, y otros indicadores financieros.\nMeteorología: para predecir el clima y los patrones climáticos.\nCiencias de la Salud: para rastrear la propagación de enfermedades o la eficacia de un tratamiento a lo largo del tiempo.\n\n\n\nClasificación de las Series de Tiempo\nLas series de tiempo pueden ser clasificadas de acuerdo a su estructura en:\n\nSeries de Tiempo Estacionarias: Estas series tienen propiedades que no dependen del tiempo. En otras palabras, poseen una media y una varianza constantes a lo largo del tiempo.\nSeries de Tiempo No Estacionarias: Estas series muestran tendencias y/o patrones estacionales. No tienen una media y/o varianza constantes a lo largo del tiempo.\nSeries de Tiempo Estacionales: Estas son un tipo especial de series no estacionarias que exhiben una tendencia estacional, es decir, un patrón que se repite en intervalos de tiempo específicos."
  },
  {
    "objectID": "data_science_ingemat/classes.html",
    "href": "data_science_ingemat/classes.html",
    "title": "Data Science for Math Engineers USACH",
    "section": "",
    "text": "Programa\nPresentación"
  },
  {
    "objectID": "data_science_ingemat/classes.html#documentos-del-curso",
    "href": "data_science_ingemat/classes.html#documentos-del-curso",
    "title": "Data Science for Math Engineers USACH",
    "section": "",
    "text": "Programa\nPresentación"
  },
  {
    "objectID": "data_science_ingemat/classes.html#clases",
    "href": "data_science_ingemat/classes.html#clases",
    "title": "Data Science for Math Engineers USACH",
    "section": "Clases",
    "text": "Clases\n\nModulo 2\n\nIntroducción\nEstacionariedad\nProphet\nComplemento\nQuiz\nProyecto"
  },
  {
    "objectID": "ejemplo.html",
    "href": "ejemplo.html",
    "title": "TITULO DEL EJEMPLO DE JUPYTER",
    "section": "",
    "text": "print('Hola')"
  },
  {
    "objectID": "machine_learning_egei/class2.html",
    "href": "machine_learning_egei/class2.html",
    "title": "DataFrames",
    "section": "",
    "text": "A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is one of the most commonly used data structures in data analysis and machine learning, especially when dealing with structured data."
  },
  {
    "objectID": "machine_learning_egei/class2.html#example-1-fictitious-data",
    "href": "machine_learning_egei/class2.html#example-1-fictitious-data",
    "title": "DataFrames",
    "section": "Example 1: Fictitious Data",
    "text": "Example 1: Fictitious Data\n\nCreating a Fictitious Data\n\nimport pandas as pd\nimport numpy as np\n\n# Creating a fictitious sales data\nnp.random.seed(42)\ndata = {\n    'Product ID': np.arange(1, 101),\n    'Product Name': ['Product_' + str(i) for i in range(1, 101)],\n    'Category': np.random.choice(['Electronics', 'Clothing', 'Groceries', 'Household'], 100),\n    'Price': np.random.uniform(10, 500, 100).round(2),\n    'Units Sold': np.random.randint(1, 100, 100),\n    'Date of Sale': pd.date_range(start='2022-01-01', periods=100, freq='D')\n}\n\n# Introducing some missing values\nfor _ in range(10):\n    data['Price'][np.random.randint(0, 100)] = np.nan\n    data['Units Sold'][np.random.randint(0, 100)] = np.nan\n\nsales_df = pd.DataFrame(data)\nsales_df.head()\n\n\n\nData Cleaning\n\n# Checking for missing values\nmissing_values = sales_df.isnull().sum()\nmissing_values\n\n\n# Handling missing values by filling with the mean of the column\nsales_df['Price'].fillna(sales_df['Price'].mean(), inplace=True)\nsales_df['Units Sold'].fillna(sales_df['Units Sold'].mean(), inplace=True)\n\n# Verifying if there are any missing values left\nsales_df.isnull().sum()\n\n\n\nData Exploration\n\n# Getting a summary of the data\nsales_summary = sales_df.describe()\nsales_summary\n\n\nimport matplotlib.pyplot as plt\n\n# Visualizing the distribution of 'Price'\nplt.figure(figsize=(10, 6))\nsales_df['Price'].hist(bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Product Prices')\nplt.xlabel('Price')\nplt.ylabel('Number of Products')\nplt.grid(False)\nplt.show()\n\n\n\nFeature Engineering\n\n# Creating a new feature 'Revenue' which is 'Price' multiplied by 'Units Sold'\nsales_df['Revenue'] = sales_df['Price'] * sales_df['Units Sold']\n\n# Creating a feature 'Month of Sale' extracted from 'Date of Sale'\nsales_df['Month of Sale'] = sales_df['Date of Sale'].dt.month\n\nsales_df[['Product Name', 'Price', 'Units Sold', 'Revenue', 'Month of Sale']].head()"
  },
  {
    "objectID": "machine_learning_egei/class2.html#example-2-fortune-data",
    "href": "machine_learning_egei/class2.html#example-2-fortune-data",
    "title": "DataFrames",
    "section": "Example 2: Fortune Data",
    "text": "Example 2: Fortune Data\n\nReading a CSV File\n\n# Importing necessary libraries\nimport pandas as pd\n\n# Reading a sample CSV file related to business and economy\n# For this example, I'll use a dataset about Fortune 500 companies\nurl = 'https://raw.githubusercontent.com/hizocar/datasets/main/fortune500.csv'\nfortune_df = pd.read_csv(url, sep=',', on_bad_lines='skip')\n\n# Displaying the first few rows of the dataset using .head()\nfortune_df.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n\nDescripción de las Columnas\n\nYear: Esta columna representa el año en el que se registraron los datos de la empresa. Es de tipo int64, lo que significa que contiene valores enteros.\nName: Esta columna contiene el nombre de la empresa. Es de tipo object, lo que generalmente indica que contiene cadenas de texto.\nRevenue: Esta columna representa los ingresos de la empresa en millones. Es de tipo float64, lo que indica que contiene valores decimales.\n\nCada columna tiene un significado específico y es esencial para el análisis de las empresas Fortune 500 y su rendimiento a lo largo de los años.\n\n# Checking the data types of each column \ndata_types = fortune_df.dtypes\ndata_types\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n\nData Cleaning: Transforming the ‘Revenue’ Column\n\n# Replacing '-' with 0\nfortune_df['Revenue'] = fortune_df['Revenue'].replace('-', '0')\n\n# Removing any remaining non-numeric characters and converting to float\nfortune_df['Revenue'] = fortune_df['Revenue'].replace('[\\$,]', '', regex=True).astype(float)\n\n# Checking the data types of each column again\ndata_types_updated = fortune_df.dtypes\ndata_types_updated\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Standardizing column names to lowercase and replacing spaces with underscores\nfortune_df.columns = fortune_df.columns.str.lower().str.replace(' ', '_')\n\n# Displaying the updated column names\nfortune_df.columns\n\nIndex(['year', 'name', 'revenue', 'rank'], dtype='object')\n\n\n\n\nData Exploration\n\n# Checking for missing values in the dataframe\nmissing_values = fortune_df.isnull().sum()\nmissing_values\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\nFrom the initial check for missing values, it appears that there are no missing values in any of the columns of the dataframe. This is great as it means we don’t have to perform any imputation or data filling for this dataset.\nNext, let’s use the .describe() method to get a summary of the distribution of the numerical data.\n\n# Using .describe() to get a summary of the numerical columns\ndata_summary = fortune_df.describe()\ndata_summary\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\nThe .describe() method provides a summary of the numerical columns in the dataframe. Here are some insights from the summary:\n\nYear: The data spans from 1955 to 2021.\nRevenue: The average revenue of the companies listed is approximately 8,488.47 million. The minimum revenue is 0 million (which might indicate missing or unreported data for some companies), and the maximum revenue is a staggering 559,151 million. The 25th percentile is 480.45 million, the median (50th percentile) is 2,184.2 million, and the 75th percentile is 7,829.15 million.\nRank: The rank ranges from 1 to 500, which is expected for a list of the top 500 companies.\n\nNext, let’s check the shape of the dataframe to understand its dimensions and then explore the unique values in the non-numerical columns.\n\n# Checking the shape of the dataframe\ndata_shape = fortune_df.shape\n\n# Checking unique values in the 'name' column\nunique_companies = fortune_df['name'].nunique()\n\ndata_shape, unique_companies\n\n((33500, 4), 2273)\n\n\nHere are some additional insights from our exploration:\n\nThe dataframe has 33,500 rows and 4 columns. This means we have data for 33,500 entries across the 4 columns.\nThe name column, which represents the names of the companies, has 2,273 unique values. This indicates that many companies have appeared on the list multiple times over the years.\n\nGiven the size of the dataset and the number of unique companies, there’s a wealth of information to explore further, such as trends over time, the distribution of revenues among the top companies, and more.\n\n# Importing necessary libraries for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Setting the style for the plots\nsns.set_style('whitegrid')\n\n# Plotting the distribution of the 'year' column\nplt.figure(figsize=(14, 6))\nsns.countplot(x='year', data=fortune_df, palette='viridis')\nplt.title('Distribution of Companies by Year')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Plotting the distribution of the 'revenue' column\nplt.figure(figsize=(14, 6))\nsns.histplot(fortune_df['revenue'], bins=50, color='blue', kde=True)\nplt.title('Distribution of Revenue')\nplt.xlabel('Revenue (in millions)')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\n\n\nNow, let’s explore the relationships between the numerical variables. One of the best ways to visualize relationships between numerical variables is by using a correlation heatmap. This will allow us to see if there are any strong linear relationships between the variables in our dataset.\n\n# Calculating the correlation matrix\ncorrelation_matrix = fortune_df.corr()\n\n# Plotting the correlation heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\nplt.title('Correlation Heatmap')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom the correlation heatmap, we can observe the following:\n\nYear and Rank: There’s a negative correlation between the year and the rank. This might indicate that over the years, the ranking criteria or the companies in the list have changed.\nRevenue and Rank: There’s also a negative correlation between revenue and rank. This is expected since a higher revenue would generally mean a higher rank (or a lower rank number) on the Fortune 500 list.\n\nIt’s important to note that correlation does not imply causation. While these variables might be correlated, it doesn’t mean that one causes the other. The heatmap provides a quick overview of potential relationships that might be worth exploring further.\n\n# Grouping the data by company and year\ngrouped_data = fortune_df.groupby(['name', 'year']).mean().reset_index()\n\n# Plotting the correlation heatmap for the grouped data\ncorrelation_matrix_grouped = grouped_data.corr()\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix_grouped, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\nplt.title('Correlation Heatmap (Grouped Data)')\nplt.tight_layout()\nplt.show()\n\n\n\n\nAfter grouping the data by company and year, the correlation heatmap for the grouped data shows similar insights as before. The negative correlation between year and rank and between revenue and rank remains consistent.\nNext, let’s visualize the revenue evolution over the years for three companies. We’ll choose ‘Walmart’, ‘Exxon Mobil’, and ‘Apple’ as our sample companies for this analysis.\n\n# Filtering data for the three chosen companies\ncompanies = ['Walmart', 'Exxon Mobil', 'Apple']\nfiltered_data = grouped_data[grouped_data['name'].isin(companies)]\n\n# Plotting the revenue evolution over the years for the three companies\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=filtered_data, x='year', y='revenue', hue='name', palette='tab10', marker='o')\nplt.title('Revenue Evolution Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Revenue (in millions)')\nplt.legend(title='Company')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "machine_learning_egei/class1.html",
    "href": "machine_learning_egei/class1.html",
    "title": "Class1: Different types of Data",
    "section": "",
    "text": "In the field of data science, we often talk about different types of data. Understanding these types is crucial for proper data analysis and interpretation. Here are some of the most common types of data:\nIn Python, we have several data types that are commonly used. Here are some of them with examples:\nPython has a variety of operators that can be used with different data types. Here are some examples:"
  },
  {
    "objectID": "machine_learning_egei/class1.html#python-data-types-exercises",
    "href": "machine_learning_egei/class1.html#python-data-types-exercises",
    "title": "Class1: Different types of Data",
    "section": "Python Data Types: Exercises",
    "text": "Python Data Types: Exercises\n\n# Exercise 1: Working with integers and floats\nnum1 = 10\nnum2 = 3\n\n# Addition\nprint('Addition:', num1 + num2)\n\n# Subtraction\nprint('Subtraction:', num1 - num2)\n\n# Multiplication\nprint('Multiplication:', num1 * num2)\n\n# Division\nprint('Division:', num1 / num2)\n\n# Floor Division\nprint('Floor Division:', num1 // num2)\n\n# Modulus\nprint('Modulus:', num1 % num2)\n\n# Exponentiation\nprint('Exponentiation:', num1 ** num2)\n\nAddition: 13\nSubtraction: 7\nMultiplication: 30\nDivision: 3.3333333333333335\nFloor Division: 3\nModulus: 1\nExponentiation: 1000\n\n\n\n# Exercise 2: Working with strings\nstr1 = 'Hello'\nstr2 = 'World'\n\n# String concatenation\nprint('Concatenation:', str1 + ' ' + str2)\n\n# String repetition\nprint('Repetition:', str1 * 3)\n\n# String slicing\nprint('Slicing:', str1[1:4])\n\n# String length\nprint('Length:', len(str1))\n\nConcatenation: Hello World\nRepetition: HelloHelloHello\nSlicing: ell\nLength: 5\n\n\n\n# Exercise 3: Working with lists\nlist1 = [1, 2, 3, 4, 5]\n\n# Accessing elements\nprint('First element:', list1[0])\nprint('Last element:', list1[-1])\n\n# List slicing\nprint('Slicing:', list1[1:4])\n\n# List length\nprint('Length:', len(list1))\n\n# Adding elements\nlist1.append(6)\nprint('After adding an element:', list1)\n\n# Removing elements\nlist1.remove(1)\nprint('After removing an element:', list1)\n\nFirst element: 1\nLast element: 5\nSlicing: [2, 3, 4]\nLength: 5\nAfter adding an element: [1, 2, 3, 4, 5, 6]\nAfter removing an element: [2, 3, 4, 5, 6]\n\n\n\n# Exercise 4: Working with dictionaries\ndict1 = {'name': 'John', 'age': 30, 'city': 'New York'}\n\n# Accessing elements\nprint('Name:', dict1['name'])\nprint('Age:', dict1['age'])\n\n# Adding elements\ndict1['job'] = 'Engineer'\nprint('After adding an element:', dict1)\n\n# Removing elements\ndel dict1['age']\nprint('After removing an element:', dict1)\n\nName: John\nAge: 30\nAfter adding an element: {'name': 'John', 'age': 30, 'city': 'New York', 'job': 'Engineer'}\nAfter removing an element: {'name': 'John', 'city': 'New York', 'job': 'Engineer'}"
  },
  {
    "objectID": "machine_learning_egei/class1.html#python-functions",
    "href": "machine_learning_egei/class1.html#python-functions",
    "title": "Class1: Different types of Data",
    "section": "Python Functions",
    "text": "Python Functions\nIn Python, a function is a block of reusable code that performs a specific task. Functions help break our program into smaller and modular chunks, making it organized and manageable. They also prevent repetition and make the code reusable.\nHere’s the basic syntax of a Python function:\ndef function_name(parameters):\n    \"\"\"docstring\"\"\"\n    statement(s)\n\nThe def keyword is a statement for defining a function in Python.\nfunction_name is a unique identifier for the function.\nparameters (arguments) through which we pass values to a function. These are optional.\nA colon (:) to mark the end of the function header.\nOptional documentation string (docstring) to describe what the function does.\nOne or more valid python statements that make up the function body. Statements must have the same indentation level (usually 4 spaces).\nAn optional return statement to return a value from the function.\n\n\n# Example of a Python function\n\ndef greet(name):\n    \"\"\"\n    This function greets the person passed in as a parameter\n    \"\"\"\n    print(f'Hello, {name}. Good morning!')\n\n# Calling the function\ngreet('John')\n\nHello, John. Good morning!\n\n\n\n# Example of a Python function with a return statement\n\ndef add_numbers(num1, num2):\n    \"\"\"\n    This function adds the two numbers passed in as parameters\n    \"\"\"\n    return num1 + num2\n\n# Calling the function and printing the result\nresult = add_numbers(5, 3)\nprint(f'The sum is {result}')\n\nThe sum is 8"
  },
  {
    "objectID": "machine_learning_egei/class1.html#exercises",
    "href": "machine_learning_egei/class1.html#exercises",
    "title": "Class1: Different types of Data",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Data Types\nGiven the following variables, convert each one to a different data type:\nnum = '123'\nstr_num = 456\nConvert num to an integer and str_num to a string. Print the new variables and their types.\n\n\nExercise 2: Functions\nWrite a function named calculate_average that takes a list of numbers as a parameter and returns their average.\nThen, call your function with the list [1, 2, 3, 4, 5] and print the result.\n\n\nExercise 3: Lists and Dictionaries\nGiven the following list of dictionaries:\ndata = [\n    {'name': 'John', 'age': 30, 'job': 'Engineer'},\n    {'name': 'Anna', 'age': 27, 'job': 'Doctor'},\n    {'name': 'Mike', 'age': 35, 'job': 'Artist'}\n]\nWrite a function named get_ages that takes the list as a parameter and returns a list of all ages. Then, call your function with the data list and print the result."
  },
  {
    "objectID": "introduction_to_python/class4.html#control-structures-in-python",
    "href": "introduction_to_python/class4.html#control-structures-in-python",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Control Structures in Python",
    "text": "Control Structures in Python\nControl structures in Python provide a way to direct the flow of the program’s execution. They allow the program to make decisions, repeat certain actions, or execute specific blocks of code based on conditions. The primary control structures in Python are:\n\nConditional Statements (if, elif, else): These statements allow the program to execute specific blocks of code based on whether a condition is true or false.\nif condition:\n    # code to execute if condition is true\nelif another_condition:\n    # code to execute if another_condition is true\nelse:\n    # code to execute if none of the above conditions are true\nLoops (for, while): Loops allow the program to repeat a block of code multiple times.\n\nfor loop: Iterates over a sequence (like a list or range) and executes the block of code for each item in the sequence.\nfor item in sequence:\n    # code to execute for each item\nwhile loop: Repeats a block of code as long as a condition is true.\nwhile condition:\n    # code to execute while condition is true\n\nControl Keywords (break, continue, pass):\n\nbreak: Exits the current loop prematurely.\ncontinue: Skips the rest of the current iteration and moves to the next iteration of the loop.\npass: A placeholder that does nothing; it’s used when a statement is syntactically required but no action is needed.\n\nfor item in sequence:\n    if condition:\n        break\n    elif another_condition:\n        continue\n    else:\n        pass\n\nUnderstanding and effectively using these control structures is fundamental to writing efficient and organized Python code.\n\n# Example of Conditional Statements\nx = 10\nif x &gt; 5:\n    print('x is greater than 5')\nelif x == 5:\n    print('x is equal to 5')\nelse:\n    print('x is less than 5')\n\n\n# Example of Loops\n\n# for loop\nprint('Example of for loop:')\nfor i in range(3):\n    print(f'Iteration {i}')\n\n# while loop\nprint('\\nExample of while loop:')\ncount = 0\nwhile count &lt; 3:\n    print(f'Count is {count}')\n    count += 1\n\n\n# Example of Control Keywords\n\n# break\nprint('Example of break:')\nfor i in range(5):\n    if i == 3:\n        break\n    print(i)\n\n# continue\nprint('\\nExample of continue:')\nfor i in range(5):\n    if i == 3:\n        continue\n    print(i)\n\n# pass\nprint('\\nExample of pass:')\nfor i in range(5):\n    if i == 3:\n        pass\n    print(i)\n\n\n# Complex example of 'if' statement\n\n# Determine if a year is a leap year\nyear = 2000\nif (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n    print(f'{year} is a leap year.')\nelse:\n    print(f'{year} is not a leap year.')\n\n\n# Complex example of 'for' loop\n\n# Calculate the factorial of a number using a for loop\nnum = 5\nfactorial = 1\nfor i in range(1, num + 1):\n    factorial *= i\nprint(f'Factorial of {num} is {factorial}')\n\n\n# Complex example of 'while' loop\n\n# Generate the Fibonacci sequence up to a certain number using a while loop\nn = 10\na, b = 0, 1\nprint('Fibonacci sequence up to', n, ':')\nwhile a &lt; n:\n    print(a, end=', ')\n    a, b = b, a+b\n\n\n# Complex example of 'break'\n\n# Find the first number in a list that is divisible by 7 and 5\nnumbers = list(range(1, 100))\nfor num in numbers:\n    if num % 7 == 0 and num % 5 == 0:\n        print(f'The first number divisible by 7 and 5 is: {num}')\n        break\n\n\n# Complex example of 'continue'\n\n# Print numbers from a list, but skip numbers that are divisible by 3\nnumbers = list(range(1, 15))\nfor num in numbers:\n    if num % 3 == 0:\n        continue\n    print(num, end=', ')\n\n\n# Complex example of 'pass'\n\n# Placeholder for future implementation of a function\ndef complex_function():\n    # TODO: Implement this function in the future\n    pass\n\nprint('Function defined, but not yet implemented.')"
  },
  {
    "objectID": "introduction_to_python/class4.html#functions-in-python",
    "href": "introduction_to_python/class4.html#functions-in-python",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Functions in Python",
    "text": "Functions in Python\nIn Python, a function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing. They are defined using the def keyword, followed by the function name and parentheses ().\nFunctions are essential in programming as they help you break down complex tasks into smaller, more manageable pieces. This not only makes the code more readable but also allows for code reuse.\nLet’s dive into some examples with different data types.\n\nFunction with Lists\n\ndef list_operations(input_list):\n    \"\"\"Perform some basic operations on a list.\"\"\"\n    # Append an element\n    input_list.append('new_element')\n    # Remove the first element\n    input_list.pop(0)\n    # Reverse the list\n    input_list.reverse()\n    return input_list\n\n# Test the function\nsample_list = [1, 2, 3, 4, 5]\nlist_operations(sample_list)\n\n\n\nFunction with Dictionaries\n\ndef dict_operations(input_dict):\n    \"\"\"Perform some basic operations on a dictionary.\"\"\"\n    # Add a new key-value pair\n    input_dict['new_key'] = 'new_value'\n    # Remove a key-value pair\n    del input_dict['key1']\n    # Get value of a key with default\n    value = input_dict.get('non_existent_key', 'default_value')\n    return input_dict, value\n\n# Test the function\nsample_dict = {'key1': 'value1', 'key2': 'value2'}\ndict_operations(sample_dict)\n\n\n\nFunction with numpy Arrays\n\nimport numpy as np\n\ndef numpy_operations(input_array):\n    \"\"\"Perform some basic operations on a numpy array.\"\"\"\n    # Square each element\n    squared_array = np.square(input_array)\n    # Calculate the mean\n    mean_value = np.mean(input_array)\n    # Reshape the array\n    reshaped_array = input_array.reshape(2, 2)\n    return squared_array, mean_value, reshaped_array\n\n# Test the function\nsample_array = np.array([1, 2, 3, 4])\nnumpy_operations(sample_array)\n\n\n\nFunction with pandas Series\n\nimport pandas as pd\n\ndef series_operations(input_series):\n    \"\"\"Perform some basic operations on a pandas Series.\"\"\"\n    # Add a new element\n    input_series['d'] = 4\n    # Remove an element\n    input_series.drop('a', inplace=True)\n    # Calculate the sum\n    sum_value = input_series.sum()\n    return input_series, sum_value\n\n# Test the function\nsample_series = pd.Series({'a': 1, 'b': 2, 'c': 3})\nseries_operations(sample_series)\n\n\n\nFunction with pandas DataFrame\n\ndef dataframe_operations(input_df):\n    \"\"\"Perform some basic operations on a pandas DataFrame.\"\"\"\n    # Add a new column\n    input_df['column3'] = [7, 8, 9]\n    # Drop a column\n    input_df.drop('column1', axis=1, inplace=True)\n    # Calculate the mean of column2\n    mean_value = input_df['column2'].mean()\n    return input_df, mean_value\n\n# Test the function\nsample_df = pd.DataFrame({'column1': [1, 2, 3], 'column2': [4, 5, 6]})\ndataframe_operations(sample_df)"
  },
  {
    "objectID": "introduction_to_python/class4.html#list-comprehension",
    "href": "introduction_to_python/class4.html#list-comprehension",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "List Comprehension",
    "text": "List Comprehension\nList comprehension is a concise way to create lists in Python. It offers a shorter syntax when you want to create a new list based on the values of an existing list or iterable. The primary motivation behind using list comprehension is to make code more readable and expressive.\nThe basic syntax is:\n[expression for item in iterable if condition]\n\nexpression is the current item in the iteration, but it is also the outcome, which can be manipulated before it ends up like a list item in the new list.\nitem is the current iteration/item.\niterable is any object that can return its elements one at a time, such as a list, a tuple, a string, etc.\ncondition is like a filter that only accepts the items that evaluate as True.\n\nList comprehensions are related to control structures, particularly loops and conditionals. In essence, a list comprehension combines a for loop and an if statement to create a new list. The for loop iterates over each element in the iterable, and the if statement filters out the unwanted elements.\nFor example, if you want to create a list of squares for even numbers from 0 to 9, you can use a list comprehension with a conditional statement to filter out odd numbers.\n\n# Using list comprehension to get squares of even numbers from 0 to 9\nsquares_with_comprehension = [x**2 for x in range(10) if x % 2 == 0]\n\n# Without using list comprehension\nsquares_without_comprehension = []\nfor x in range(10):\n    if x % 2 == 0:\n        squares_without_comprehension.append(x**2)\n\nsquares_with_comprehension, squares_without_comprehension\n\n\n# Example 1: Get all vowels from a string\n\n# Using list comprehension\ninput_string = 'Hello, World!'\nvowels_with_comprehension = [char for char in input_string if char.lower() in 'aeiou']\n\n# Without using list comprehension\nvowels_without_comprehension = []\nfor char in input_string:\n    if char.lower() in 'aeiou':\n        vowels_without_comprehension.append(char)\n\nvowels_with_comprehension, vowels_without_comprehension\n\n\n# Example 2: Get numbers from a list that are greater than 5\n\n# Using list comprehension\nnumbers = [1, 3, 7, 9, 10, 12]\ngreater_than_five_with_comprehension = [num for num in numbers if num &gt; 5]\n\n# Without using list comprehension\ngreater_than_five_without_comprehension = []\nfor num in numbers:\n    if num &gt; 5:\n        greater_than_five_without_comprehension.append(num)\n\ngreater_than_five_with_comprehension, greater_than_five_without_comprehension\n\n\n# Example 3: Get even indices from a list\n\n# Using list comprehension\nitems = ['a', 'b', 'c', 'd', 'e', 'f']\neven_indices_with_comprehension = [items[i] for i in range(len(items)) if i % 2 == 0]\n\n# Without using list comprehension\neven_indices_without_comprehension = []\nfor i in range(len(items)):\n    if i % 2 == 0:\n        even_indices_without_comprehension.append(items[i])\n\neven_indices_with_comprehension, even_indices_without_comprehension"
  },
  {
    "objectID": "introduction_to_python/class4.html#exercises",
    "href": "introduction_to_python/class4.html#exercises",
    "title": "Class 4: Control Structures, functions and list comprehension",
    "section": "Exercises",
    "text": "Exercises\nTo test your understanding of the concepts covered in this notebook, try the following exercises:\n\nEasy\n\nControl Structures: Write a program that prints numbers from 1 to 10 using a for loop.\nFunctions: Create a function that takes a list of numbers and returns their average.\n\n\n\nMedium\n\nList Comprehension: Use list comprehension to create a list of squares for numbers from 1 to 20 that are divisible by 3.\nFunctions with Data Structures: Create a function that takes a dictionary and returns a list of keys whose values are strings.\n\n\n\nHard\n\nComplex Control Structures: Write a program that prints the Fibonacci sequence up to the 10th term using a while loop.\nAdvanced Functions: Create a function that takes a pandas DataFrame and returns a new DataFrame with only the numeric columns, and each value squared."
  },
  {
    "objectID": "introduction_to_python/class5.html#set-logic",
    "href": "introduction_to_python/class5.html#set-logic",
    "title": "Class 5: Data Manipulation I",
    "section": "Set Logic",
    "text": "Set Logic\nIn mathematics, a set is a collection of distinct objects, considered as an object in its own right. The operations associated with sets include:\n\nUnion: Represents all elements that are in either set. Denoted as \\(A \\cup B\\).\nIntersection: Represents all elements that are common to both sets. Denoted as \\(A \\cap B\\).\nDifference: Represents all elements that are in the first set but not in the second. Denoted as \\(A - B\\).\nComplement: Represents all elements that are not in the given set. Denoted as \\(\\overline{A}\\).\n\nThese operations form the basic foundation for set theory and are crucial when dealing with collections of data."
  },
  {
    "objectID": "introduction_to_python/class5.html#pandas-merge-function",
    "href": "introduction_to_python/class5.html#pandas-merge-function",
    "title": "Class 5: Data Manipulation I",
    "section": "Pandas Merge Function",
    "text": "Pandas Merge Function\nThe merge function in pandas is used to combine two DataFrames based on common columns or indices. It’s similar to SQL JOIN operations. The key parameters include:\n\nright: The DataFrame to merge with.\non: Column(s) that should be used to join the DataFrames. These columns should exist in both DataFrames.\nleft_on: Columns from the left DataFrame to use as keys.\nright_on: Columns from the right DataFrame to use as keys.\n\nLet’s create two example DataFrames to demonstrate the use of merge with right_on and left_on.\n\nimport pandas as pd\n\n# Creating two example DataFrames\nstudents = pd.DataFrame({\n    'student_id': [101, 102, 103],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\ncourses = pd.DataFrame({\n    'registration_no': [101, 102, 104],\n    'course': ['Math', 'Physics', 'Chemistry']\n})\n\n# Merging the DataFrames using merge with right_on and left_on\nmerged_df = students.merge(courses, left_on='student_id', right_on='registration_no', how='inner')\n\nmerged_df\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#outer-join-vs-inner-join",
    "href": "introduction_to_python/class5.html#outer-join-vs-inner-join",
    "title": "Class 5: Data Manipulation I",
    "section": "Outer Join vs Inner Join",
    "text": "Outer Join vs Inner Join\nWhen merging or joining DataFrames in pandas, it’s essential to understand the type of join you want to perform. The two most common types are outer join and inner join:\n\nOuter Join: This type of join returns all the rows when there is a match in one of the DataFrames. Therefore, it returns all the rows from the left DataFrame and all the rows from the right DataFrame. If there is no match, the missing side will contain NaN.\nInner Join: This type of join returns only the rows where there is a match in both DataFrames. If there’s no match in one of the DataFrames, that row will not be in the result.\n\nTo illustrate the difference between these two types of joins, let’s consider an example. Suppose we have a DataFrame of authors and their books, and another DataFrame of books and their prices. We want to merge these DataFrames based on the book titles. Let’s first create and display these DataFrames.\n\n# Creating two example DataFrames\nauthors = pd.DataFrame({\n    'book_title': ['The Great Gatsby', 'Moby Dick', 'Pride and Prejudice'],\n    'author': ['F. Scott Fitzgerald', 'Herman Melville', 'Jane Austen']\n})\n\nprices = pd.DataFrame({\n    'book_title': ['The Great Gatsby', 'Moby Dick', 'War and Peace'],\n    'price': [10, 15, 20]\n})\n\nauthors, prices\n\n(            book_title               author\n 0     The Great Gatsby  F. Scott Fitzgerald\n 1            Moby Dick      Herman Melville\n 2  Pride and Prejudice          Jane Austen,\n          book_title  price\n 0  The Great Gatsby     10\n 1         Moby Dick     15\n 2     War and Peace     20)\n\n\n\n# Performing an outer join\nouter_join_result = authors.merge(prices, on='book_title', how='outer')\n\n# Performing an inner join\ninner_join_result = authors.merge(prices, on='book_title', how='inner')\n\nouter_join_result, inner_join_result\n\n(            book_title               author  price\n 0     The Great Gatsby  F. Scott Fitzgerald   10.0\n 1            Moby Dick      Herman Melville   15.0\n 2  Pride and Prejudice          Jane Austen    NaN\n 3        War and Peace                  NaN   20.0,\n          book_title               author  price\n 0  The Great Gatsby  F. Scott Fitzgerald     10\n 1         Moby Dick      Herman Melville     15)"
  },
  {
    "objectID": "introduction_to_python/class5.html#pivot-tables-transposition-and-lambda-functions",
    "href": "introduction_to_python/class5.html#pivot-tables-transposition-and-lambda-functions",
    "title": "Class 5: Data Manipulation I",
    "section": "Pivot Tables, Transposition, and Lambda Functions",
    "text": "Pivot Tables, Transposition, and Lambda Functions\nIn this section, we’ll delve into some advanced data manipulation techniques in pandas, including pivot tables, transposition, and the use of lambda functions.\n\nPivot Tables\nA pivot table is a data summarization tool used in spreadsheets and other data visualization software. It aggregates a table of data by one or more keys, arranging the data in a rectangle with some of the group keys along the rows and some along the columns. In pandas, the pivot_table function provides this functionality.\nThe primary parameters of the pivot_table function are:\n\nvalues: Column(s) to aggregate.\nindex: Column(s) to use as the row of the pivot table.\ncolumns: Column(s) to use as the columns of the pivot table.\naggfunc: Aggregation function to use on the data. It can be ‘sum’, ‘mean’, etc. or a custom function.\n\nLet’s look at two practical examples to understand the use of pivot tables.\n\n# Creating two example DataFrames for pivot_table demonstration\n\n# Example 1: Sales data\nsales_data = pd.DataFrame({\n    'Date': ['2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02'],\n    'Product': ['A', 'B', 'A', 'B'],\n    'Sales': [100, 150, 110, 160]\n})\n\n# Pivot table to get total sales for each product across dates\npivot_sales = sales_data.pivot_table(values='Sales', index='Date', columns='Product', aggfunc='sum')\n\n# Example 2: Student grades\ngrades_data = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Alice', 'Bob'],\n    'Subject': ['Math', 'Math', 'History', 'History'],\n    'Grade': [85, 90, 78, 88]\n})\n\n# Pivot table to get grades for each student across subjects\npivot_grades = grades_data.pivot_table(values='Grade', index='Student', columns='Subject')\n\npivot_sales, pivot_grades\n\n(Product       A    B\n Date                \n 2022-01-01  100  150\n 2022-01-02  110  160,\n Subject  History  Math\n Student               \n Alice         78    85\n Bob           88    90)\n\n\n\n\nTransposition\nTransposition is a fundamental operation in linear algebra and data manipulation. In the context of DataFrames, transposition refers to switching the rows and columns with each other. In pandas, you can transpose a DataFrame using the transpose() method or its shorthand .T.\nTransposing can be useful in various scenarios, such as when you want to change the orientation of your data for visualization or when you want to treat columns as rows for certain operations.\nLet’s look at an example to understand the use of transposition.\n\n# Example DataFrame for transposition\ndf_transpose = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'London', 'Paris']\n})\n\n# Transposing the DataFrame\ntransposed_df = df_transpose.transpose()\n\ndf_transpose, transposed_df\n\n(      Name  Age      City\n 0    Alice   25  New York\n 1      Bob   30    London\n 2  Charlie   35     Paris,\n              0       1        2\n Name     Alice     Bob  Charlie\n Age         25      30       35\n City  New York  London    Paris)\n\n\n\n\nLambda Functions with Pivot Tables\nLambda functions are small, anonymous functions that can have any number of arguments, but can only have one expression. They are useful for performing simple operations without the need to define a full function. In the context of pivot_table, lambda functions can be used as custom aggregation functions.\nFor instance, you might want to aggregate data in a way that’s not directly supported by built-in functions. In such cases, a lambda function can be handy.\nLet’s look at an example where we use a lambda function with pivot_table to calculate the range (difference between max and min) of values.\n\n# Example DataFrame for lambda function with pivot_table\ndf_lambda = pd.DataFrame({\n    'Category': ['A', 'A', 'B', 'B', 'A', 'B'],\n    'Values': [10, 15, 20, 25, 30, 35]\n})\n\n# Using pivot_table with lambda function to calculate the range of values for each category\npivot_lambda = df_lambda.pivot_table(index='Category', values='Values', aggfunc=lambda x: x.max() - x.min())\n\npivot_lambda\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#data-cleaning-and-transformation",
    "href": "introduction_to_python/class5.html#data-cleaning-and-transformation",
    "title": "Class 5: Data Manipulation I",
    "section": "Data Cleaning and Transformation",
    "text": "Data Cleaning and Transformation\nOne of the essential steps in the data preprocessing pipeline is data cleaning and transformation. This step ensures that the data is of high quality and is ready for analysis. A common issue encountered during this phase is the presence of duplicate records.\n\nDuplicates in a DataFrame\nDuplicates refer to rows in a DataFrame that are identical to another row. These can arise due to various reasons such as data entry errors, merging datasets, or not handling data updates correctly. Duplicates can lead to incorrect analysis results, so it’s crucial to identify and handle them appropriately.\nIn pandas, you can use the drop_duplicates() method to remove duplicate rows and the duplicated() method to identify them. The subset parameter allows you to consider certain columns for identifying duplicates.\nLet’s dive into an entertaining example to understand the use of these functions.\n\n# Example DataFrame with duplicate records\ndf_duplicates = pd.DataFrame({\n    'Superhero': ['Spider-Man', 'Iron Man', 'Spider-Man', 'Thor', 'Iron Man', 'Hulk'],\n    'Real Name': ['Peter Parker', 'Tony Stark', 'Peter Parker', 'Thor Odinson', 'Tony Stark', 'Bruce Banner'],\n    'City': ['New York', 'New York', 'New York', 'Asgard', 'Los Angeles', 'New York']\n})\n\n# Identifying duplicate rows\nduplicated_rows = df_duplicates[df_duplicates.duplicated(subset=['Superhero', 'Real Name'])]\n\n# Removing duplicate rows\ndf_cleaned = df_duplicates.drop_duplicates(subset=['Superhero', 'Real Name'])\n\ndf_duplicates, duplicated_rows, df_cleaned\n\n(    Superhero     Real Name         City\n 0  Spider-Man  Peter Parker     New York\n 1    Iron Man    Tony Stark     New York\n 2  Spider-Man  Peter Parker     New York\n 3        Thor  Thor Odinson       Asgard\n 4    Iron Man    Tony Stark  Los Angeles\n 5        Hulk  Bruce Banner     New York,\n     Superhero     Real Name         City\n 2  Spider-Man  Peter Parker     New York\n 4    Iron Man    Tony Stark  Los Angeles,\n     Superhero     Real Name      City\n 0  Spider-Man  Peter Parker  New York\n 1    Iron Man    Tony Stark  New York\n 3        Thor  Thor Odinson    Asgard\n 5        Hulk  Bruce Banner  New York)\n\n\n\n\nOriginal DataFrame (Superhero Records)\nThis DataFrame contains records of superheroes, their real names, and cities. As you can observe, there are some duplicate entries in the data.\n\ndf_duplicates\n\n\n\nIdentified Duplicate Rows\nUsing the duplicated() function with the subset parameter, we can identify rows where both the ‘Superhero’ and ‘Real Name’ columns are duplicated. Here are the rows that have been identified as duplicates:\n\nduplicated_rows\n\n\n\nCleaned DataFrame (After Removing Duplicates)\nAfter identifying the duplicate rows, we can use the drop_duplicates() function with the subset parameter to remove these rows and obtain a cleaned DataFrame. Here’s the DataFrame after removing the duplicates:\n\ndf_cleaned"
  },
  {
    "objectID": "introduction_to_python/class5.html#missing-data",
    "href": "introduction_to_python/class5.html#missing-data",
    "title": "Class 5: Data Manipulation I",
    "section": "Missing Data",
    "text": "Missing Data\nIn the realm of data analysis, missing data, often represented as NaN (Not a Number), is a common occurrence. It can arise due to various reasons, such as data entry errors, unrecorded observations, or during data processing. Handling missing data is crucial as it can significantly impact the results of your analysis.\n\nIdentifying Missing Data\nBefore handling missing data, it’s essential to identify them in your dataset. In pandas, the isna() or isnull() methods can be used to detect missing values. These methods return a DataFrame of the same shape as the original, but with True where the data is missing and False where it’s not.\n\n# Example DataFrame with missing data\ndf_missing = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, np.nan, 35, 40],\n    'City': ['New York', 'Los Angeles', np.nan, 'Chicago']\n})\n\n# Identifying missing data\ndf_missing.isna()\n\n\n\nRemoving Missing Data\nOnce you’ve identified missing data, you might decide to remove them from your dataset. Pandas provides two main methods for this:\n\ndropna(): This method allows you to drop rows or columns containing missing data.\nfillna(): Instead of dropping missing data, you can also replace them with a specific value or a computed value (like mean, median, etc.).\n\n\n# Removing rows with missing data\ndf_no_missing_rows = df_missing.dropna()\n\n# Removing columns with missing data\ndf_no_missing_columns = df_missing.dropna(axis=1)\n\ndf_no_missing_rows, df_no_missing_columns\n\n\n\nFilling Missing Data\nInstead of removing missing data, another approach is to fill or replace them. The fillna() method in pandas allows you to replace missing values with a specific value, forward fill, backward fill, or even a computed value like the mean or median of the column.\n\n# Filling missing data with a specific value\ndf_filled_value = df_missing.fillna('Unknown')\n\n# Filling missing data with mean of the column\ndf_filled_mean = df_missing.copy()\ndf_filled_mean['Age'] = df_filled_mean['Age'].fillna(df_filled_mean['Age'].mean())\n\ndf_filled_value, df_filled_mean"
  },
  {
    "objectID": "introduction_to_python/class5.html#apply-function",
    "href": "introduction_to_python/class5.html#apply-function",
    "title": "Class 5: Data Manipulation I",
    "section": "Apply Function",
    "text": "Apply Function\nThe apply() function in pandas is a powerful tool that allows you to apply a function along the axis of a DataFrame (either rows or columns). It’s especially useful when you want to perform custom operations that are not readily available through built-in pandas functions.\nLet’s explore some entertaining examples to understand the versatility of the apply() function.\n\n# Example 1: Capitalizing names\ndf_names = pd.DataFrame({\n    'Name': ['alice', 'bob', 'charlie', 'david']\n})\n\ndf_names['Capitalized Name'] = df_names['Name'].apply(lambda x: x.capitalize())\ndf_names\n\n\n# Example 2: Calculating the length of strings\ndf_phrases = pd.DataFrame({\n    'Phrase': ['Hello World', 'Pandas is awesome', 'Apply function rocks!', 'Data Science']\n})\n\ndf_phrases['Length'] = df_phrases['Phrase'].apply(len)\ndf_phrases\n\n\n# Example 3: Categorizing based on age\ndf_age = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [15, 28, 35, 42]\n})\n\ndef age_category(age):\n    if age &lt; 18:\n        return 'Minor'\n    elif 18 &lt;= age &lt; 35:\n        return 'Young Adult'\n    else:\n        return 'Adult'\n\ndf_age['Category'] = df_age['Age'].apply(age_category)\ndf_age"
  },
  {
    "objectID": "introduction_to_python/class5.html#differences-between-apply-and-applymap",
    "href": "introduction_to_python/class5.html#differences-between-apply-and-applymap",
    "title": "Class 5: Data Manipulation I",
    "section": "Differences between apply and applymap",
    "text": "Differences between apply and applymap\nBoth apply and applymap are essential tools in pandas for element-wise operations, but they serve different purposes and are used in different contexts.\n\napply: This function is used on both Series and DataFrame. When used on a Series, it applies a function to each element. When used on a DataFrame, it can apply a function along the axis (either rows or columns).\napplymap: This function is exclusive to DataFrames and is used to apply a function to each element of the DataFrame.\n\nLet’s delve into some examples to understand when to use one over the other.\n\n# Example DataFrame\ndf_example = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Using apply on a Series\nseries_squared = df_example['A'].apply(lambda x: x**2)\n\n# Using apply on a DataFrame to get the sum of each column\ncolumn_sum = df_example.apply(sum, axis=0)\n\n# Using applymap to square each element of the DataFrame\ndf_squared = df_example.applymap(lambda x: x**2)\n\n\nseries_squared\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ncolumn_sum\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ndf_squared\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#replacing-data",
    "href": "introduction_to_python/class5.html#replacing-data",
    "title": "Class 5: Data Manipulation I",
    "section": "Replacing Data",
    "text": "Replacing Data\nIn data manipulation, there are often scenarios where you need to replace certain values in your dataset. The replace() method in pandas is a versatile tool that allows you to replace values in a DataFrame or Series.\nLet’s start with a basic example of the replace() method.\n\n# Example DataFrame\ndf_replace = pd.DataFrame({\n    'Fruit': ['Apple', 'Banana', 'Cherry', 'Apple'],\n    'Color': ['Red', 'Yellow', 'Red', 'Green']\n})\n\n# Using replace to change 'Apple' to 'Mango'\ndf_replaced = df_replace.replace('Apple', 'Mango')\ndf_replaced"
  },
  {
    "objectID": "introduction_to_python/class5.html#regular-expressions-regex",
    "href": "introduction_to_python/class5.html#regular-expressions-regex",
    "title": "Class 5: Data Manipulation I",
    "section": "Regular Expressions (Regex)",
    "text": "Regular Expressions (Regex)\nRegular expressions, often abbreviated as regex, are sequences of characters that define a search pattern. They are incredibly powerful for string matching and manipulation. In pandas, you can use regex with the replace() method to perform more complex replacements.\nLet’s delve into the world of regex and explore some examples to understand its capabilities.\n\n# Example DataFrame with phone numbers\ndf_phone = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Phone': ['123-456-7890', '(123) 456-7890', '123.456.7890']\n})\n\n# Using regex to standardize phone number format\ndf_phone_standardized = df_phone.copy()\ndf_phone_standardized['Phone'] = df_phone_standardized['Phone'].replace(r'\\D', '', regex=True)\ndf_phone_standardized['Phone'] = df_phone_standardized['Phone'].replace(r'(\\d{3})(\\d{3})(\\d{4})', r'\\1-\\2-\\3', regex=True)\n\n\ndf_phone\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\ndf_phone_standardized\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class5.html#deep-dive-into-regular-expressions-regex",
    "href": "introduction_to_python/class5.html#deep-dive-into-regular-expressions-regex",
    "title": "Class 5: Data Manipulation I",
    "section": "Deep Dive into Regular Expressions (Regex)",
    "text": "Deep Dive into Regular Expressions (Regex)\nRegular expressions (regex) are a powerful tool for working with text data. They allow you to create search patterns using sequences of characters, which can be used for string matching and manipulation. The true power of regex lies in its flexibility and the wide range of patterns it can match.\nLet’s break down some of the fundamental components of regex and explore various examples to understand its capabilities.\n\nBasic Regex Patterns\n\n. : Matches any character except a newline.\n^ : Matches the start of a string.\n$ : Matches the end of a string.\n* : Matches 0 or more repetitions of the preceding character or group.\n+ : Matches 1 or more repetitions of the preceding character or group.\n? : Matches 0 or 1 repetition of the preceding character or group.\n\\d : Matches any decimal digit. Equivalent to [0-9].\n\\D : Matches any non-digit character.\n\\w : Matches any alphanumeric character or underscore. Equivalent to [a-zA-Z0-9_].\n\\W : Matches any non-alphanumeric character.\n\\s : Matches any whitespace character (spaces, tabs, line breaks).\n\\S : Matches any non-whitespace character.\n\nLet’s see some of these patterns in action with examples.\n\nimport re\n\n# Sample text\ntext = 'My phone number is 123-456-7890 and my zip code is 98765.'\n\n# Extracting phone number using regex\nphone_pattern = r'\\d{3}-\\d{3}-\\d{4}'\nphone_match = re.search(phone_pattern, text)\nphone_number = phone_match.group() if phone_match else None\n\n# Extracting zip code using regex\nzip_pattern = r'\\d{5}$'\nzip_match = re.search(zip_pattern, text)\nzip_code = zip_match.group() if zip_match else None\n\nphone_number, zip_code\n\nIn the example above, we used the following regex patterns:\n\n\\d{3}-\\d{3}-\\d{4}: This pattern matches a phone number format where there are three digits, followed by a hyphen, another three digits, another hyphen, and finally four digits.\n\\d{5}$: This pattern matches five digits at the end of a string, which is a common format for zip codes in the US.\n\nNow, let’s explore more advanced regex patterns and their applications.\n\n\nAdvanced Regex Patterns\n\nCharacter Sets [...]: Matches any one of the characters inside the square brackets. For example, [aeiou] matches any vowel.\nNegated Character Sets [^...]: Matches any character that is not inside the square brackets. For example, [^aeiou] matches any non-vowel character.\nQuantifiers {m,n}: Matches between m and n repetitions of the preceding character or group. For example, a{2,4} matches ‘aa’, ‘aaa’, or ‘aaaa’.\nNon-capturing Groups (?:...): Groups multiple patterns together without creating a capture group.\nPositive Lookahead (?=...): Asserts that what directly follows the current position in the string matches the pattern inside the lookahead, but doesn’t consume any characters.\nNegative Lookahead (?!...): Asserts that what directly follows the current position in the string does not match the pattern inside the lookahead.\n\nLet’s see some of these advanced patterns in action with examples.\n\n# Sample text for regex examples\ntext_advanced = 'The price is $100.00, but there's a discount of 10% if you pay within 5 days.'\n\n# Extracting all prices using regex (character sets)\nprice_pattern = r'\\$[0-9]+\\.[0-9]{2}'\nprices = re.findall(price_pattern, text_advanced)\n\n# Extracting words that don't start with a vowel (negated character sets)\nnon_vowel_pattern = r'\\b[^aeiouAEIOU \\d][a-zA-Z]*'\nnon_vowel_words = re.findall(non_vowel_pattern, text_advanced)\n\n# Using positive lookahead to find 'discount' if it's followed by '10%'\nlookahead_pattern = r'discount(?= of 10%)'\nlookahead_match = re.search(lookahead_pattern, text_advanced)\nlookahead_word = lookahead_match.group() if lookahead_match else None\n\nprices, non_vowel_words, lookahead_word\n\nIn the examples above, we utilized various advanced regex patterns:\n\nCharacter Sets: The pattern \\$[0-9]+\\.[0-9]{2} matches dollar amounts. It looks for a dollar sign, followed by one or more digits, a period, and exactly two digits after the period.\nNegated Character Sets: The pattern \\b[^aeiouAEIOU \\d][a-zA-Z]* matches words that don’t start with a vowel. It looks for word boundaries (\\b), followed by any character that is not a vowel or a digit, and then any sequence of alphabetic characters.\nPositive Lookahead: The pattern discount(?= of 10%) matches the word ‘discount’ only if it’s directly followed by ’ of 10%’. The positive lookahead (?=...) checks for the presence of a pattern without consuming any characters, allowing us to match based on what follows our main pattern.\n\nRegular expressions are a vast topic, and there’s a lot more to explore. The key is to practice and experiment with different patterns to become proficient."
  },
  {
    "objectID": "introduction_to_python/class5.html#exercises",
    "href": "introduction_to_python/class5.html#exercises",
    "title": "Class 5: Data Manipulation I",
    "section": "Exercises",
    "text": "Exercises\nNow that you’ve learned about various data manipulation techniques in pandas and regular expressions, it’s time to test your knowledge! Below are 10 exercises that cover the topics discussed in this notebook. Try to solve each one to reinforce your understanding.\n\nMerging DataFrames: Given two DataFrames, df1 with columns ['A', 'B'] and df2 with columns ['B', 'C'], merge them on column ‘B’.\nPivot Tables: Create a pivot table from a DataFrame df with columns ['Date', 'Product', 'Sales'] to show the total sales for each product over time.\nTranspose: Transpose a DataFrame df and explain what happens to its indices and columns.\nLambda Functions: Use a lambda function to square all the values in a DataFrame column ‘X’.\nRegex Phone Numbers: Extract all phone numbers from a text string. Consider phone numbers to be in the formats 123-456-7890, (123) 456-7890, and 123.456.7890.\nHandling Duplicates: Identify and remove any duplicate rows in a DataFrame df based on columns ‘A’ and ‘B’.\nHandling Missing Data: Replace all NaN values in a DataFrame df with the mean of the column they are in.\nApply Function: Apply a function that calculates the length of each string in the ‘Name’ column of a DataFrame df.\nReplace with Regex: Replace all occurrences of the word ‘color’ (case-insensitive) in a text string with the word ‘hue’ using regex.\nAdvanced Regex: Extract all email addresses from a text string. Consider email addresses to be in the format name@domain.com."
  },
  {
    "objectID": "introduction_to_python/classes.html",
    "href": "introduction_to_python/classes.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Class 1: Getting to know python | \nClass 2: Numpy Arrays | \nClass 3: Pandas Dataframe | \nClass 4: Control Structures, functions and list comprehension | \nClass 5: Data Manipulation I |"
  },
  {
    "objectID": "introduction_to_python/class1.html",
    "href": "introduction_to_python/class1.html",
    "title": "Class 1: Getting to Know Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language known for its readability and simplicity. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is widely used in data analysis, web development, machine learning, and artificial intelligence. Its extensive library support and vibrant community make it a popular choice for both beginners and experienced developers.\nHere is a simple example of Python code that takes an input and produces an output:\n# Input\nname = input('What is your name? ')\n\n# Output\nprint(f'Hello, {name}!')"
  },
  {
    "objectID": "introduction_to_python/class1.html#data-types-in-python",
    "href": "introduction_to_python/class1.html#data-types-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Data Types in Python",
    "text": "Data Types in Python\nPython has several built-in data types. Here are some of the most common ones:\n\nIntegers (int): Whole numbers, such as 3, 4, or 100.\nFloating point numbers (float): Numbers with a decimal point, such as 2.3 or 4.6.\nStrings (str): Sequences of characters, such as ‘hello’ or ‘python’.\nBooleans (bool): True or false values.\n\nLet’s see some examples:\n\n# Integer\nx = 10\nprint(type(x))\n\n# Float\ny = 3.14\nprint(type(y))\n\n# String\nz = 'Hello, Python!'\nprint(type(z))\n\n# Boolean\nw = True\nprint(type(w))"
  },
  {
    "objectID": "introduction_to_python/class1.html#lists-in-python",
    "href": "introduction_to_python/class1.html#lists-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Lists in Python",
    "text": "Lists in Python\nA list in Python is an ordered collection of items that can be of any type. Lists are very flexible and can be modified after they are created (they are mutable).\nHere is an example of a list:\n\n# List of integers\nnumbers = [1, 2, 3, 4, 5]\nprint(numbers)\n\n# List of different types\nmixed = [1, 'two', 3.0, True]\nprint(mixed)"
  },
  {
    "objectID": "introduction_to_python/class1.html#dictionaries-in-python",
    "href": "introduction_to_python/class1.html#dictionaries-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Dictionaries in Python",
    "text": "Dictionaries in Python\nA dictionary in Python is an unordered collection of items. Each item in a dictionary has a key and a corresponding value. Dictionaries are mutable, meaning they can be changed after they are created.\nHere is an example of a dictionary:\n\n# Dictionary\nperson = {\n    'name': 'John',\n    'age': 30,\n    'is_married': True\n}\nprint(person)"
  },
  {
    "objectID": "introduction_to_python/class1.html#comparing-lists-and-dictionaries",
    "href": "introduction_to_python/class1.html#comparing-lists-and-dictionaries",
    "title": "Class 1: Getting to Know Python",
    "section": "Comparing Lists and Dictionaries",
    "text": "Comparing Lists and Dictionaries\nLists and dictionaries are both versatile data structures in Python, but they are used in different scenarios.\nLists are ordered collections of items, and they are best when the order of items matters. You can use lists when you have a collection of items that you want to keep in a specific order, or when you want to easily add or remove items from the collection.\nDictionaries are unordered collections of key-value pairs, and they are best when you need to associate values with keys, so you can look them up efficiently by key. Dictionaries are ideal for data that is labeled, where each item can be accessed by a unique key (like a product code or name).\nLet’s see some examples:\n\n# List example\nfruits = ['apple', 'banana', 'cherry']\nprint(fruits)\n\n# Add an item to the list\nfruits.append('date')\nprint(fruits)\n\n# Dictionary example\nfruit_colors = {\n    'apple': 'red',\n    'banana': 'yellow',\n    'cherry': 'red'\n}\nprint(fruit_colors)\n\n# Add an item to the dictionary\nfruit_colors['date'] = 'brown'\nprint(fruit_colors)\n\nIt is possible to convert a list to a dictionary and vice versa, but the way you do it depends on the structure of your data.\nFor example, if you have a list of pairs, you can convert it to a dictionary using the dict() function. And if you have a dictionary, you can convert it to a list of pairs using the items() method.\nLet’s see some examples:\n\n# List of pairs\npairs = [('one', 1), ('two', 2), ('three', 3)]\n\n# Convert list to dictionary\ndict_from_list = dict(pairs)\nprint(dict_from_list)\n\n# Dictionary\ndictionary = {'one': 1, 'two': 2, 'three': 3}\n\n# Convert dictionary to list\nlist_from_dict = list(dictionary.items())\nprint(list_from_dict)"
  },
  {
    "objectID": "introduction_to_python/class1.html#operations-and-methods-in-python",
    "href": "introduction_to_python/class1.html#operations-and-methods-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Operations and Methods in Python",
    "text": "Operations and Methods in Python\nIn Python, an operation is an action that is carried out on one or more values. For example, the + operation adds two numbers together, and the * operation multiplies them.\nA method is a function that is associated with a particular type of object. For example, the count method can be used on a list to count the number of times a particular value appears in the list, and the upper method can be used on a string to convert all the characters to uppercase.\nThe len function is a built-in Python function that returns the length of a sequence, such as a list or a string.\nLet’s see some examples:\n\n# Operations\nprint(3 + 4)  # Addition\nprint(3 * 4)  # Multiplication\n\n# Methods\nnumbers = [1, 2, 3, 2, 1]\nprint(numbers.count(2))  # Count method\n\ntext = 'hello'\nprint(text.upper())  # Upper method\n\n# Len function\nprint(len(numbers))  # Length of a list\nprint(len(text))  # Length of a string"
  },
  {
    "objectID": "introduction_to_python/class1.html#combining-different-data-types-with-and",
    "href": "introduction_to_python/class1.html#combining-different-data-types-with-and",
    "title": "Class 1: Getting to Know Python",
    "section": "Combining Different Data Types with + and *",
    "text": "Combining Different Data Types with + and *\nIn Python, the + and * operators can be used with different types of data, but the behavior can be different depending on the types of the operands.\nWhen used with integers (int), the + operator performs addition and the * operator performs multiplication.\nWhen used with strings (str), the + operator concatenates the strings, and the * operator repeats the string a certain number of times.\nHowever, trying to use + or * with an integer and a string will result in a TypeError.\nLet’s see some examples:\n\n# Addition with integers\nprint(3 + 4)\n\n# Multiplication with integers\nprint(3 * 4)\n\n# Concatenation with strings\nprint('hello' + 'world')\n\n# Repetition with strings\nprint('hello' * 3)\n\n# Trying to add an integer and a string\ntry:\n    print(3 + 'hello')\nexcept TypeError as e:\n    print(e)\n\n# Trying to multiply an integer and a string\ntry:\n    print(3 * 'hello')\nexcept TypeError as e:\n    print(e)\n\n7\n12\nhelloworld\nhellohellohello\nunsupported operand type(s) for +: 'int' and 'str'\nhellohellohello"
  },
  {
    "objectID": "introduction_to_python/class1.html#libraries-in-python",
    "href": "introduction_to_python/class1.html#libraries-in-python",
    "title": "Class 1: Getting to Know Python",
    "section": "Libraries in Python",
    "text": "Libraries in Python\nIn Python, a library is a collection of modules, which are files containing Python code that define functions, classes, and variables that you can use once the module is imported.\nPython comes with a standard library that includes many useful modules, and there are also many third-party libraries available that provide additional functionality.\nYou can import a module using the import statement. Once a module is imported, you can use its functions and variables by prefixing them with the module name and a dot.\nIf a library is not installed, you can install it using the pip tool, which is the package installer for Python.\nLet’s see some examples:\n\n# Importing a module from the standard library\nimport math\n\n# Using a function from the math module\nprint(math.sqrt(16))\n\n# Note: The following code is commented out because it requires user interaction and might not work in this notebook interface.\n# But you can run it in your local Python environment.\n\n# Installing a third-party library (uncomment to run)\n# !pip install numpy\n\n4.0\n\n\nIf you only need a specific function from a module, you can import just that function using the from ... import ... statement. This allows you to use the function directly without prefixing it with the module name.\nLet’s see an example:\n\n# Importing a specific function from the math module\nfrom math import sqrt\n\n# Using the function directly\nprint(sqrt(16))\n\n4.0"
  },
  {
    "objectID": "introduction_to_python/class1.html#google-colab",
    "href": "introduction_to_python/class1.html#google-colab",
    "title": "Class 1: Getting to Know Python",
    "section": "Google Colab",
    "text": "Google Colab\nGoogle Colab is a cloud-based Python development environment that provides a platform for anyone to write and execute Python code through the browser. It is especially useful for machine learning, data analysis, and education.\n\nAdvantages\n\nNo setup required\nFree access to GPUs\nEasy sharing\n\nWhether you’re a student, a data scientist, or an AI researcher, Colab can make your work easier. You can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser.\n\n\nDisadvantages\n\nInternet connection is required\nLimited resources\n\nDespite its advantages, Google Colab does have some limitations. It requires an internet connection to save and run notebooks, and the available resources (RAM and disk space) are limited.\nTo start a new notebook in Google Colab, you can follow this link."
  },
  {
    "objectID": "introduction_to_python/class1.html#exercises",
    "href": "introduction_to_python/class1.html#exercises",
    "title": "Class 1: Getting to Know Python",
    "section": "Exercises",
    "text": "Exercises\n\nData Types: Write a Python function that takes two inputs, determines their data types, and returns a message indicating the data type of each input.\nList Operations: Given a list of numbers, write a Python function that returns a new list containing the square of each number. Use a loop in your solution.\nDictionary Operations: Write a Python function that takes a dictionary and a key as input, checks if the key is in the dictionary, and returns a message indicating whether the key was found.\nString Methods: Write a Python function that takes a string as input and returns a new string with the first letter of each word capitalized.\nLibrary Usage: Use the math library to write a Python function that takes a number as input and returns the square root of the number."
  },
  {
    "objectID": "introduction_to_python/class3.html",
    "href": "introduction_to_python/class3.html",
    "title": "Dataframes in Python",
    "section": "",
    "text": "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. It is generally the most commonly used pandas object. You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects. It is generally the most commonly used pandas object.\nDataFrames can be created in various ways, but for this example, we’ll create a DataFrame from a dictionary of pandas Series.\nimport pandas as pd\n\ndata = {\n    'apples': pd.Series([3, 2, 0, 1]),\n    'oranges': pd.Series([0, 3, 7, 2])\n}\n\ndf = pd.DataFrame(data)\n\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#dataframe-indexing",
    "href": "introduction_to_python/class3.html#dataframe-indexing",
    "title": "Dataframes in Python",
    "section": "DataFrame Indexing",
    "text": "DataFrame Indexing\nIndexing in pandas means simply selecting particular rows and columns of data from a DataFrame. Indexes can be used to select specific rows and columns that you want to manipulate. They can also be used to modify the structure of the DataFrame itself, for example, by adding rows or columns.\nLet’s explore some examples of how to work with DataFrame indexes.\n\n# Set a column as the index\ndf_indexed = df.set_index('apples')\ndf_indexed\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Reset the index\ndf_reset = df_indexed.reset_index()\ndf_reset\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#creating-dataframes",
    "href": "introduction_to_python/class3.html#creating-dataframes",
    "title": "Dataframes in Python",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\nPandas DataFrames can be created in various ways. Some of the most common methods are: from a list, from a dictionary, from a list of dictionaries, and from a NumPy array. Let’s explore examples of each.\n\n# Creating a DataFrame from a list\nlist_data = [['Alex',10],['Bob',12],['Clarke',13]]\ndf_list = pd.DataFrame(list_data, columns=['Name','Age'])\ndf_list\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a dictionary\ndict_data = {'Name':['Tom', 'Nick', 'John'], 'Age':[20, 21, 19]}\ndf_dict = pd.DataFrame(dict_data)\ndf_dict\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a list of dictionaries\nlist_dict_data = [{'a': 1, 'b': 2},{'a': 3, 'b': 4, 'c': 5}]\ndf_list_dict = pd.DataFrame(list_dict_data)\ndf_list_dict\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a DataFrame from a NumPy array\nimport numpy as np\n\nnumpy_data = np.array([[1, 2], [3, 4]])\ndf_numpy = pd.DataFrame(numpy_data, columns=['Column1', 'Column2'])\ndf_numpy\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#dataframe-operations-and-methods",
    "href": "introduction_to_python/class3.html#dataframe-operations-and-methods",
    "title": "Dataframes in Python",
    "section": "DataFrame Operations and Methods",
    "text": "DataFrame Operations and Methods\nPandas DataFrames offer a wide range of operations and methods that can be used to manipulate and analyze data. In this section, we’ll explore how to create new columns, how to create columns from other columns through operations, and how to combine DataFrames using the concatenate method.\n\n# Creating a new column\ndf['bananas'] = pd.Series([1, 0, 2, 4])\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating a column from other columns\ndf['total_fruits'] = df['apples'] + df['oranges'] + df['bananas']\ndf\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Creating another DataFrame to concatenate\ndf2 = pd.DataFrame({'apples': [5, 3], 'oranges': [2, 4], 'bananas': [7, 6]}, index=[4, 5])\n\n# Concatenating DataFrames\ndf_concat = pd.concat([df, df2])\ndf_concat\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\nThe ignore_index Parameter\nWhen concatenating DataFrames, pandas provides an ignore_index parameter. If ignore_index is set to True, the resulting DataFrame will have a new integer index, ignoring the original indices of the concatenated DataFrames. Let’s see an example.\n\n# Concatenating DataFrames with ignore_index\ndf_concat_ignore_index = pd.concat([df, df2], ignore_index=True)\ndf_concat_ignore_index\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#exploring-a-dataframe",
    "href": "introduction_to_python/class3.html#exploring-a-dataframe",
    "title": "Dataframes in Python",
    "section": "Exploring a DataFrame",
    "text": "Exploring a DataFrame\nPandas provides several methods that are useful for quickly summarizing and gaining insights from your data. In this section, we’ll explore the value_counts, unique, nunique, and describe methods. Let’s first create a new DataFrame for these examples.\n\n# Creating a new DataFrame\ndata = {\n    'A': np.random.randint(1, 10, 20),\n    'B': np.random.choice(['red', 'green', 'blue'], 20),\n    'C': np.random.normal(0, 1, 20),\n    'D': np.random.choice(['cat', 'dog', 'rabbit'], 20),\n    'E': np.random.randint(1, 100, 20)\n}\ndf_explore = pd.DataFrame(data)\ndf_explore\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# value_counts method\ndf_explore['B'].value_counts()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# unique method\ndf_explore['D'].unique()\n\narray(['rabbit', 'dog', 'cat'], dtype=object)\n\n\n\n# nunique method\ndf_explore['A'].nunique()\n\n9\n\n\n\n# describe method\ndf_explore.describe()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#sorting-and-ranking-in-a-dataframe",
    "href": "introduction_to_python/class3.html#sorting-and-ranking-in-a-dataframe",
    "title": "Dataframes in Python",
    "section": "Sorting and Ranking in a DataFrame",
    "text": "Sorting and Ranking in a DataFrame\nPandas provides several methods for sorting and ranking data in a DataFrame. In this section, we’ll explore the sort_values, iloc, and loc methods. The sort_values method sorts a DataFrame by one or more columns, while iloc and loc are used for indexing and selecting data.\n\n# sort_values method\ndf_explore_sorted = df_explore.sort_values(by='A')\ndf_explore_sorted.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# iloc method\ndf_explore_iloc = df_explore_sorted.iloc[0:5]\ndf_explore_iloc\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# loc method\ndf_explore_loc = df_explore_sorted.loc[df_explore_sorted['B'] == 'red']\ndf_explore_loc.head()\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#boolean-indexing",
    "href": "introduction_to_python/class3.html#boolean-indexing",
    "title": "Dataframes in Python",
    "section": "Boolean Indexing",
    "text": "Boolean Indexing\nBoolean indexing is a powerful tool that allows you to select data that meets certain conditions. This can be done using comparison operators (&gt;, &lt;, ==) and logical operators (& for ‘and’, | for ‘or’). Let’s see some examples.\n\n# Boolean indexing with '&gt;'\ndf_explore[df_explore['A'] &gt; 5]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '&lt;'\ndf_explore[df_explore['A'] &lt; 5]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '=='\ndf_explore[df_explore['B'] == 'red']\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '&' (and)\ndf_explore[(df_explore['A'] &gt; 5) & (df_explore['B'] == 'red')]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json\n\n\n\n# Boolean indexing with '|' (or)\ndf_explore[(df_explore['A'] &lt; 5) | (df_explore['B'] == 'red')]\n\nUnable to display output for mime type(s): application/vnd.dataresource+json"
  },
  {
    "objectID": "introduction_to_python/class3.html#exporting-dataframes",
    "href": "introduction_to_python/class3.html#exporting-dataframes",
    "title": "Dataframes in Python",
    "section": "Exporting DataFrames",
    "text": "Exporting DataFrames\nPandas provides several methods to export a DataFrame to different file formats. This can be very useful when you want to save your data for later use or to share it with others. In this section, we’ll explore how to export a DataFrame to CSV, Excel, and JSON formats.\n\n# Exporting to CSV\n# df_explore.to_csv('df_explore.csv', index=False)\n\n\n# Exporting to Excel\n# df_explore.to_excel('df_explore.xlsx', index=False)\n\n\n# Exporting to JSON\n# df_explore.to_json('df_explore.json', orient='records')"
  },
  {
    "objectID": "introduction_to_python/class3.html#exercises",
    "href": "introduction_to_python/class3.html#exercises",
    "title": "Dataframes in Python",
    "section": "Exercises",
    "text": "Exercises\nNow that we have learned about pandas DataFrames, let’s put our knowledge into practice with some exercises. These exercises cover all the topics we have discussed in this notebook and vary in difficulty from easy to hard.\n\nExercise 1 (Easy)\nCreate a DataFrame from a dictionary with keys ‘Name’, ‘Age’, and ‘City’. The ‘Name’ column should contain five different names, the ‘Age’ column should contain ages between 20 and 40, and the ‘City’ column should contain the names of five different cities.\n\n\nExercise 2 (Medium)\nGiven the DataFrame created in Exercise 1, perform the following operations:\n\nSet the ‘Name’ column as the index of the DataFrame.\nReset the index of the DataFrame.\nSelect only the rows where ‘Age’ is greater than 30.\n\n\n\nExercise 3 (Medium)\nCreate a DataFrame with 10 rows and 3 columns named ‘A’, ‘B’, and ‘C’. The ‘A’ column should contain random integers between 1 and 10, the ‘B’ column should contain random floats between 0 and 1, and the ‘C’ column should contain the string ‘random’ for all rows. Then, export this DataFrame to a CSV file named ‘random.csv’.\n\n\nExercise 4 (Hard)\nGiven the DataFrame created in Exercise 3, perform the following operations:\n\nCreate a new column ‘D’ that is the result of ‘A’ divided by ‘B’.\nReplace all ‘inf’ values in column ‘D’ with ‘NaN’.\nDrop all rows with ‘NaN’ values.\nReset the index of the DataFrame without adding a new ‘index’ column.\n\n\n\nExercise 5 (Hard)\nGiven a DataFrame with 100 rows and 5 columns named ‘A’, ‘B’, ‘C’, ‘D’, and ‘E’. All columns should contain random integers between 1 and 100. Perform the following operations:\n\nSelect only the rows where ‘A’ is greater than 50 and ‘B’ is less than 50.\nFrom the resulting DataFrame, select only the ‘C’, ‘D’, and ‘E’ columns.\nCalculate the sum of the ‘C’, ‘D’, and ‘E’ columns for each row (you should end up with a Series).\nFind the row (index) with the highest sum."
  },
  {
    "objectID": "introduction_to_python/class2.html",
    "href": "introduction_to_python/class2.html",
    "title": "Introduction to NumPy",
    "section": "",
    "text": "NumPy, which stands for Numerical Python, is a foundational package for numerical computations in Python. It provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays."
  },
  {
    "objectID": "introduction_to_python/class2.html#examples-of-numpy-arrays",
    "href": "introduction_to_python/class2.html#examples-of-numpy-arrays",
    "title": "Introduction to NumPy",
    "section": "Examples of NumPy Arrays",
    "text": "Examples of NumPy Arrays\nIn NumPy, arrays are the main way to store and manipulate data. Let’s look at some examples of creating and working with arrays.\n\nimport numpy as np\n\n# Creating a 1-dimensional array\narr_1d = np.array([1, 2, 3, 4, 5])\nprint('1D Array:', arr_1d)\n\n# Creating a 2-dimensional array\narr_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint('\\n2D Array:')\nprint(arr_2d)\n\n# Creating a 3-dimensional array\narr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint('\\n3D Array:')\nprint(arr_3d)"
  },
  {
    "objectID": "introduction_to_python/class2.html#array-operations",
    "href": "introduction_to_python/class2.html#array-operations",
    "title": "Introduction to NumPy",
    "section": "Array Operations",
    "text": "Array Operations\nArrays in NumPy can be operated on in a variety of ways. One common operation is element-wise addition. Let’s see what happens when we try to add arrays of different shapes.\n\n# Adding two 1D arrays of the same shape\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\nsum_1d = arr1 + arr2\nprint('Sum of two 1D arrays:', sum_1d)\n\n# Adding two 2D arrays of the same shape\narr3 = np.array([[1, 2], [3, 4]])\narr4 = np.array([[5, 6], [7, 8]])\nsum_2d = arr3 + arr4\nprint('\\nSum of two 2D arrays:')\nprint(sum_2d)\n\n# Trying to add arrays of different shapes (This will raise an error)\ntry:\n    sum_diff_shapes = arr1 + arr3\nexcept ValueError as e:\n    print('\\nError when trying to add arrays of different shapes:', e)"
  },
  {
    "objectID": "introduction_to_python/class2.html#numpy-functions-and-attributes",
    "href": "introduction_to_python/class2.html#numpy-functions-and-attributes",
    "title": "Introduction to NumPy",
    "section": "NumPy Functions and Attributes",
    "text": "NumPy Functions and Attributes\nNumPy provides a wide range of functions and attributes to perform operations on arrays. Let’s explore some of the commonly used ones.\n\n# Sample array for demonstration\narr = np.array([1, 5, 3, 7, 2, 8, 4, 6])\n\n# logical_and and logical_or\ncondition_and = np.logical_and(arr &gt; 2, arr &lt; 7)\ncondition_or = np.logical_or(arr &lt; 3, arr &gt; 6)\nprint('logical_and result:', condition_and)\nprint('logical_or result:', condition_or)\n\n# .shape attribute\nprint('\\nShape of the array:', arr.shape)\n\n# argmin() and argmax()\nprint('\\nIndex of minimum value:', arr.argmin())\nprint('Index of maximum value:', arr.argmax())\n\n# mean, median, std, var\nprint('\\nMean of the array:', np.mean(arr))\nprint('Median of the array:', np.median(arr))\nprint('Standard deviation of the array:', np.std(arr))\nprint('Variance of the array:', np.var(arr))\n\n# percentile\nprint('\\n25th percentile:', np.percentile(arr, 25))\nprint('75th percentile:', np.percentile(arr, 75))\n\n\nExplanation of the functions and attributes used:\n\nlogical_and: This function performs element-wise logical AND operation. It returns an array of the same shape as the input arrays, with True where the condition is met and False otherwise.\nlogical_or: Similar to logical_and, this function performs element-wise logical OR operation.\n.shape: This attribute returns a tuple representing the dimensions of the array.\nargmin(): Returns the index of the minimum value in the array.\nargmax(): Returns the index of the maximum value in the array.\nmean: Computes the arithmetic mean of the array elements.\nmedian: Returns the median of the array elements.\nstd: Computes the standard deviation of the array elements.\nvar: Computes the variance of the array elements.\npercentile: Computes the specified percentile of the array elements."
  },
  {
    "objectID": "introduction_to_python/class2.html#additional-numpy-functions",
    "href": "introduction_to_python/class2.html#additional-numpy-functions",
    "title": "Introduction to NumPy",
    "section": "Additional NumPy Functions",
    "text": "Additional NumPy Functions\nNumPy offers a plethora of functions for various mathematical and statistical operations. Here are some more functions that haven’t been covered yet:\n\ndot: Computes the dot product of two arrays.\ncross: Computes the cross product of two arrays.\nlinspace: Returns evenly spaced numbers over a specified range.\narange: Returns evenly spaced values within a given interval.\nreshape: Gives a new shape to an array without changing its data.\nconcatenate: Joins two or more arrays along an existing axis.\nsplit: Divides an array into multiple sub-arrays.\nsin, cos, tan: Trigonometric functions.\nexp: Computes the exponential of all elements in the input array.\nlog, log10, log2: Natural logarithm, base-10 logarithm, and base-2 logarithm.\nsqrt: Returns the non-negative square-root of an array, element-wise.\nunique: Finds the unique elements of an array.\nsort: Returns a sorted copy of an array.\n\n\n# Sample arrays for demonstration\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# dot\ndot_product = np.dot(a, b)\nprint('Dot product of a and b:', dot_product)\n\n# cross\ncross_product = np.cross(a, b)\nprint('\\nCross product of a and b:', cross_product)\n\n# linspace\nlinspace_array = np.linspace(0, 10, 5)\nprint('\\nLinspace array:', linspace_array)\n\n# arange\narange_array = np.arange(0, 10, 2)\nprint('\\nArange array:', arange_array)\n\n# reshape\nreshaped_array = np.arange(9).reshape(3, 3)\nprint('\\nReshaped array:')\nprint(reshaped_array)\n\n# concatenate\nconcatenated_array = np.concatenate((a, b))\nprint('\\nConcatenated array:', concatenated_array)\n\n# split\nsplit_arrays = np.split(concatenated_array, 2)\nprint('\\nSplit arrays:', split_arrays)\n\n# sin, cos, tan\nprint('\\nSin of a:', np.sin(a))\nprint('Cos of a:', np.cos(a))\nprint('Tan of a:', np.tan(a))\n\n# exp\nprint('\\nExponential of a:', np.exp(a))\n\n# log, log10, log2\nprint('\\nNatural logarithm of a:', np.log(a))\nprint('Base-10 logarithm of a:', np.log10(a))\nprint('Base-2 logarithm of a:', np.log2(a))\n\n# sqrt\nprint('\\nSquare-root of a:', np.sqrt(a))\n\n# unique\nunique_array = np.unique(np.array([1, 2, 2, 3, 3, 3]))\nprint('\\nUnique elements:', unique_array)\n\n# sort\nsorted_array = np.sort(np.array([3, 1, 2]))\nprint('\\nSorted array:', sorted_array)"
  },
  {
    "objectID": "introduction_to_python/class2.html#exercises",
    "href": "introduction_to_python/class2.html#exercises",
    "title": "Introduction to NumPy",
    "section": "Exercises",
    "text": "Exercises\nTo test your understanding of the concepts covered in this notebook, try out the following exercises:\n\nEasy\n\nArray Creation: Create a 1-dimensional NumPy array containing the numbers from 1 to 10.\nArray Reshaping: Take the array you created in the previous exercise and reshape it into a 2x5 matrix.\n\n\n\nMedium\n\nArray Operations: Given two arrays a = np.array([1, 2, 3, 4]) and b = np.array([5, 6, 7, 8]), compute the dot product and the element-wise sum.\nArray Functions: For the array c = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100]), compute the mean, median, and standard deviation. Also, find the 25th and 75th percentiles.\n\n\n\nHard\n\nLogical Operations: Given the array d = np.array([3, 7, 1, 10, 5, 8, 2, 9, 4, 6]), use logical operations to find all numbers in the array that are greater than 3 and less than 8. Then, compute the sum of these numbers."
  },
  {
    "objectID": "personal/academica.html",
    "href": "personal/academica.html",
    "title": "Academic Experience",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "machine_learning_egei/classes.html",
    "href": "machine_learning_egei/classes.html",
    "title": "Machine Learning with Business Application UTFSM",
    "section": "",
    "text": "Program\nProgram\n\n\nClass\nClass 1: Different type of Data | \nClass 2: Data Manipulation |"
  },
  {
    "objectID": "machine_learning_egei/program.html",
    "href": "machine_learning_egei/program.html",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Course title: Machine Learning – Business Applications\nECTS: 5\nSemester: 3\nLocation: Universidad Técnica Federico Santa María (Chile)\nCompulsory course: YES – Track GEME - Globalisation and Emerging Market Economies\nLecturer & Contact: Sebastián Azócar M. Master’s in Data Science\nEmail: hizocar@gmail.com\nPrerequisites: Applied Econometrics I and II\n\n\nThe general objective of the course is that students acquire the knowledge and practice necessary to Machine learning (ML). This is a branch of computer science that uses algorithms to mimic the way humans learn. In this course, we will analyze the different techniques and statistical methods used in ML to make predictions with Business Applications.\n\n\n\nThe methodology of the course is focused on learning by doing, so the individual work of each student is key (a study load of at least 3 hours per week is assumed), and each student is required to read the required material before each class.\n\n\n\n\n\n\n\nA. Data and Decision Making\n\nDifferent Types of Data\nData manipulation\n\n\n\n\n\n\nB.1 Machine Learning Models\n\nWhat is a ML Model?\nFitting a Model\nKNN\nPolynomial Regression\nOverfitting and Underfitting: Bias versus Variance\nThe Cost Function\nThe Training Error\nThe Test Error\n\nB.2 The Machine Learning Pipeline\n\nThe Bias-Variance Trade-Off\nCross-Validation\nApplying the machine learning pipeline\n\n\n\n\n\n\nC.1 Logistic Regression\n\nWhat is classification?\nTechnique and Methodology\nMeasuring the Model Performance\nThe ROC Curve\n\nC.2 Generative Models\n\nBasic Concepts\nThe Naïve Bayesian Classifier\nText Classification\nNLP Application: Measuring Text Sentiment\n\n\n\n\n\n\nD.1 Tree based Methods\n\nStructure of decision trees\nTypes of tree-based methods\nLoss functions\nTree pruning\nRegression Trees\nClassification Trees\n\nD.2 Ensemble Methods\n\nBasic Concepts\nTechniques and Methodology\nBagging\nRandom Forests\nBoosting\n\n\n\n\n\n\nE.1 Variable Selection\n\nApplications to Variable Selections\nTechniques and Methodology\nBest subset selection\nStepwise, Backward and Forward Selection\n\nE.2 Shrinkage Methods\n\nShrinkage versus Selection\nLASSO Regression\nRIDGE Regression\nElastic Net Regression\n\n\n\n\n\n\nF.1 Dimension Reduction\n\nUnlabeled data\nPrincipal Components\nApplication of PCA\n\nF.2 Clustering\n\nk-means clustering\nHierarchical clustering\nAdvantages and Limitations\nPractical Application\n\n\n\n\n\n\nG.1 Neural Networks\n\nBasic concepts\nArtificial neural networks (ANNs)\nThe simple perceptron\nStructure of the ANN\nMethods\n\nG.2 Neural Networks Implementation\n\nIntroduction\nAdvantages and Limitations\nTraining the Model\nModel Optimization\n\n\n\n\n\n\n\n\n\nBreiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231.\nBurgess, M. (2018). This is how Netflix’s secret recommendation system works. Wired.\nCastañón, J. (10). Machine Learning Methods that Every Data Scientist Should Know, 2019.\nPant, A. (2019). Introduction to Machine Learning for Beginners. Preuzeto, 19, 2021.\nZhang (2018). Data Types From A Machine Learning Perspective With Examples.\n\n\n\n\n\nAgarwal, A. (2018). Polynomial Regression\nKoehrsen, W. (2018). Overfitting vs. Underfitting: A Complete Example\nGupta, P. (2017). Cross-Validation in Machine Learning\n\n\n\n\n\nISLR sections 6.1 (Subset selection), 6.2 (Shrinkage methods)\nLesson 4: Variable Selection\nLesson 5: Shrinkage Methods\nDeol, G. (2019)\n\n\n\n\n\nISLR sections 4.1, 4.2, 4.3, 4.6.2\nLesson 9.1: Logistic Regression\nAsiri, S.(2018)\n\n\n\n\n\nISLR Chapter 8\nLesson 11: Tree-based Methods\n[Analytics Vidha (2016). Tree Based Algorithms: A Complete Tutorial from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-s"
  },
  {
    "objectID": "machine_learning_egei/program.html#course-syllabus",
    "href": "machine_learning_egei/program.html#course-syllabus",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Course title: Machine Learning – Business Applications\nECTS: 5\nSemester: 3\nLocation: Universidad Técnica Federico Santa María (Chile)\nCompulsory course: YES – Track GEME - Globalisation and Emerging Market Economies\nLecturer & Contact: Sebastián Azócar M. Master’s in Data Science\nEmail: hizocar@gmail.com\nPrerequisites: Applied Econometrics I and II\n\n\nThe general objective of the course is that students acquire the knowledge and practice necessary to Machine learning (ML). This is a branch of computer science that uses algorithms to mimic the way humans learn. In this course, we will analyze the different techniques and statistical methods used in ML to make predictions with Business Applications.\n\n\n\nThe methodology of the course is focused on learning by doing, so the individual work of each student is key (a study load of at least 3 hours per week is assumed), and each student is required to read the required material before each class."
  },
  {
    "objectID": "machine_learning_egei/program.html#course-contents",
    "href": "machine_learning_egei/program.html#course-contents",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "A. Data and Decision Making\n\nDifferent Types of Data\nData manipulation\n\n\n\n\n\n\nB.1 Machine Learning Models\n\nWhat is a ML Model?\nFitting a Model\nKNN\nPolynomial Regression\nOverfitting and Underfitting: Bias versus Variance\nThe Cost Function\nThe Training Error\nThe Test Error\n\nB.2 The Machine Learning Pipeline\n\nThe Bias-Variance Trade-Off\nCross-Validation\nApplying the machine learning pipeline\n\n\n\n\n\n\nC.1 Logistic Regression\n\nWhat is classification?\nTechnique and Methodology\nMeasuring the Model Performance\nThe ROC Curve\n\nC.2 Generative Models\n\nBasic Concepts\nThe Naïve Bayesian Classifier\nText Classification\nNLP Application: Measuring Text Sentiment\n\n\n\n\n\n\nD.1 Tree based Methods\n\nStructure of decision trees\nTypes of tree-based methods\nLoss functions\nTree pruning\nRegression Trees\nClassification Trees\n\nD.2 Ensemble Methods\n\nBasic Concepts\nTechniques and Methodology\nBagging\nRandom Forests\nBoosting\n\n\n\n\n\n\nE.1 Variable Selection\n\nApplications to Variable Selections\nTechniques and Methodology\nBest subset selection\nStepwise, Backward and Forward Selection\n\nE.2 Shrinkage Methods\n\nShrinkage versus Selection\nLASSO Regression\nRIDGE Regression\nElastic Net Regression\n\n\n\n\n\n\nF.1 Dimension Reduction\n\nUnlabeled data\nPrincipal Components\nApplication of PCA\n\nF.2 Clustering\n\nk-means clustering\nHierarchical clustering\nAdvantages and Limitations\nPractical Application\n\n\n\n\n\n\nG.1 Neural Networks\n\nBasic concepts\nArtificial neural networks (ANNs)\nThe simple perceptron\nStructure of the ANN\nMethods\n\nG.2 Neural Networks Implementation\n\nIntroduction\nAdvantages and Limitations\nTraining the Model\nModel Optimization"
  },
  {
    "objectID": "machine_learning_egei/program.html#readings-literature",
    "href": "machine_learning_egei/program.html#readings-literature",
    "title": "Machine Learning – Business Applications",
    "section": "",
    "text": "Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231.\nBurgess, M. (2018). This is how Netflix’s secret recommendation system works. Wired.\nCastañón, J. (10). Machine Learning Methods that Every Data Scientist Should Know, 2019.\nPant, A. (2019). Introduction to Machine Learning for Beginners. Preuzeto, 19, 2021.\nZhang (2018). Data Types From A Machine Learning Perspective With Examples.\n\n\n\n\n\nAgarwal, A. (2018). Polynomial Regression\nKoehrsen, W. (2018). Overfitting vs. Underfitting: A Complete Example\nGupta, P. (2017). Cross-Validation in Machine Learning\n\n\n\n\n\nISLR sections 6.1 (Subset selection), 6.2 (Shrinkage methods)\nLesson 4: Variable Selection\nLesson 5: Shrinkage Methods\nDeol, G. (2019)\n\n\n\n\n\nISLR sections 4.1, 4.2, 4.3, 4.6.2\nLesson 9.1: Logistic Regression\nAsiri, S.(2018)\n\n\n\n\n\nISLR Chapter 8\nLesson 11: Tree-based Methods\n[Analytics Vidha (2016). Tree Based Algorithms: A Complete Tutorial from Scratch (in R & Python)](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-s"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi 👋, I’m Sebastián Azócar\n",
    "section": "",
    "text": "Hi 👋, I’m Sebastián Azócar\n\n\nData Scientist and Teacher\n\n\n\n\nPerfil\n\n\n\n👋 Hi there, I’m a Mathematical Engineer and hold a Master’s degree in Data Science\n🏠 I’m currently living in Santiago, Chile.\n🌐 I love traveling and visiting different countries.\n📚 I’m always learning and have a special interest in data analysis.\n📫 How to reach me: hizocar@gmail.com\n\n\nConnect with me:\n\n\n  \n\n\nLanguages:\n\n\n           \n     \n\nML and Data Analysis:\n\n\n              \n\nCloud:"
  },
  {
    "objectID": "data_science_ingemat/programa.html",
    "href": "data_science_ingemat/programa.html",
    "title": "Programa",
    "section": "",
    "text": "Objetivos Generales:\n\nDesarrollar habilidades avanzadas en ciencia de datos aplicadas en Python.\nFamiliarizar a los estudiantes con técnicas avanzadas de aprendizaje automático.\nCapacitar a los estudiantes en el uso de herramientas avanzadas para el análisis de datos en tiempo real.\nPromover la capacidad de los estudiantes para aplicar técnicas de análisis de datos en problemas del mundo real.\n\n\n\nMódulo 1: Sistemas de Recomendación\n\nIntroducción a los sistemas de recomendación.\nFamiliarización con las técnicas de filtrado colaborativo y basado en contenido.\nImplementación de sistemas de recomendación utilizando Python y la biblioteca de ciencia de datos de Pandas.\nEvaluación del rendimiento de los sistemas de recomendación.\n\n\n\nMódulo 2: Series de Tiempo con Prophet\n\nIntroducción a las series de tiempo y sus aplicaciones en la ciencia de datos.\nFamiliarización con la biblioteca Prophet de Facebook para el análisis de series de tiempo.\nImplementación de técnicas avanzadas de análisis de series de tiempo, como la descomposición y la modelización de tendencias y estacionalidad.\nEvaluación del rendimiento del modelo.\n\n\n\nMódulo 3: Teoría de Colas aplicado en Python\n\nIntroducción a la teoría de colas y sus aplicaciones en la ciencia de datos.\nFamiliarización con las técnicas avanzadas de análisis de colas, como la ley de Little y la teoría de colas de Jackson.\nImplementación de modelos de teoría de colas utilizando Python y la biblioteca de ciencia de datos de NumPy.\nEvaluación del rendimiento del modelo.\n\n\n\nMetodología y estructura del curso:\n\nEl curso tendrá una duración de un semestre.\nCada semana, se realizará un quiz corto para evaluar el progreso de los estudiantes.\nLos estudiantes trabajarán en un proyecto de ciencia de datos a lo largo del semestre.\nLa evaluación será con una proporción del 50% promedio de quiz y un 50% proyecto."
  },
  {
    "objectID": "data_science_ingemat/modulo2/prophet.html",
    "href": "data_science_ingemat/modulo2/prophet.html",
    "title": "Tema 3: Prophet",
    "section": "",
    "text": "Prophet es una biblioteca de Python desarrollada por Facebook que se utiliza para la predicción de series temporales. Prophet es especialmente útil para los casos de uso que tienen patrones estacionales fuertes y varias temporadas de datos históricos. Prophet también es robusto ante los datos faltantes y los cambios en la tendencia, y normalmente maneja bien los valores atípicos.\nLa biblioteca Prophet intenta capturar la tendencia y la estacionalidad al ajustar un modelo aditivo donde las observaciones no negativas se describen como la suma de los componentes. El objetivo de Prophet es hacer que las predicciones de series temporales sean escalables y automatizadas.\nA continuación, enumeraremos y describiremos brevemente algunas de las principales funciones que ofrece la biblioteca Prophet.\n\n\n\nProphet(): Esta es la clase principal de la biblioteca Prophet. Se utiliza para crear un objeto de Prophet que luego se puede ajustar a los datos de la serie temporal y se utiliza para hacer predicciones.\nfit(): Este método se utiliza para ajustar el modelo Prophet a los datos de la serie temporal. Se debe llamar antes de hacer predicciones.\nmake_future_dataframe(): Este método se utiliza para crear un DataFrame de Pandas de fechas futuras para las cuales hacer predicciones.\npredict(): Este método se utiliza para hacer predicciones. Se puede llamar después de ajustar el modelo Prophet a los datos de la serie temporal.\nplot() y plot_components(): Estos métodos se utilizan para visualizar las predicciones y los componentes del modelo Prophet.\nadd_seasonality(): Este método se utiliza para agregar estacionalidad personalizada al modelo Prophet.\nadd_country_holidays(): Este método se utiliza para agregar las vacaciones de un país específico al modelo Prophet.\n\n\n\n\nCrea una jupyter notebook con una data sintetica u otra a elección que simule una serie de tiempo. Usa cada función expuesta anteriormente y comenta los resultados"
  },
  {
    "objectID": "data_science_ingemat/modulo2/prophet.html#tarea-2",
    "href": "data_science_ingemat/modulo2/prophet.html#tarea-2",
    "title": "Tema 3: Prophet",
    "section": "",
    "text": "Crea una jupyter notebook con una data sintetica u otra a elección que simule una serie de tiempo. Usa cada función expuesta anteriormente y comenta los resultados"
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html",
    "href": "data_science_ingemat/modulo2/proyecto2.html",
    "title": "Proyecto",
    "section": "",
    "text": "En este proyecto, se espera que apliquen sus conocimientos de series de tiempo y el uso de la biblioteca Prophet para realizar pronósticos. Debe encontrar un conjunto de datos con más de tres años de historia para su análisis.\n\n\n\n\nRealizar un análisis exploratorio de los datos para entender las características de la serie de tiempo.\nPreprocesar los datos si es necesario para prepararlos para Prophet.\nEntrenar un modelo Prophet en los datos y realizar pronósticos.\nEvaluar la precisión de sus pronósticos.\nInterpretar los componentes de su modelo (tendencia, estacionalidad).\n\n\n\n\n\nAsegúrese de dividir sus datos en conjuntos de entrenamiento y prueba para evaluar la precisión de sus pronósticos.\nConsidere la posibilidad de ajustar los hiperparámetros de Prophet para mejorar su modelo.\nRecuerde que Prophet puede manejar datos faltantes, pero debe decidir si imputar estos datos es la mejor opción para su situación.\nLa entrega del proyecto debe realizarse a través de un repositorio de GitHub. Asegúrese de que su proyecto esté bien comentado, para esto use el archivo ReadMe.\nLa fecha límite para la entrega del proyecto es el lunes 31 de julio. Las presentaciones se llevarán a cabo hasta el 5 de agosto.\n\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas.\nCorrecta aplicación de Prophet y uso de sus funcionalidades.\nPrecisión de los pronósticos.\nInterpretación de los componentes del modelo.\nCalidad de la presentación de sus resultados.\n\n\n\n\n\nAjuste de la flexibilidad de la tendencia con el parámetro changepoint_prior_scale.\nAjuste de la estacionalidad con add_seasonality y seasonality_mode.\nUso de add_country_holidays para incluir efectos de vacaciones.\nUso de make_future_dataframe para generar fechas futuras para pronósticos.\nInterpretación de los componentes del modelo con plot_components."
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html#proyecto-de-series-de-tiempo-con-prophet",
    "href": "data_science_ingemat/modulo2/proyecto2.html#proyecto-de-series-de-tiempo-con-prophet",
    "title": "Proyecto",
    "section": "",
    "text": "En este proyecto, se espera que apliquen sus conocimientos de series de tiempo y el uso de la biblioteca Prophet para realizar pronósticos. Debe encontrar un conjunto de datos con más de tres años de historia para su análisis.\n\n\n\n\nRealizar un análisis exploratorio de los datos para entender las características de la serie de tiempo.\nPreprocesar los datos si es necesario para prepararlos para Prophet.\nEntrenar un modelo Prophet en los datos y realizar pronósticos.\nEvaluar la precisión de sus pronósticos.\nInterpretar los componentes de su modelo (tendencia, estacionalidad).\n\n\n\n\n\nAsegúrese de dividir sus datos en conjuntos de entrenamiento y prueba para evaluar la precisión de sus pronósticos.\nConsidere la posibilidad de ajustar los hiperparámetros de Prophet para mejorar su modelo.\nRecuerde que Prophet puede manejar datos faltantes, pero debe decidir si imputar estos datos es la mejor opción para su situación.\nLa entrega del proyecto debe realizarse a través de un repositorio de GitHub. Asegúrese de que su proyecto esté bien comentado, para esto use el archivo ReadMe.\nLa fecha límite para la entrega del proyecto es el lunes 31 de julio. Las presentaciones se llevarán a cabo hasta el 5 de agosto.\n\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas.\nCorrecta aplicación de Prophet y uso de sus funcionalidades.\nPrecisión de los pronósticos.\nInterpretación de los componentes del modelo.\nCalidad de la presentación de sus resultados.\n\n\n\n\n\nAjuste de la flexibilidad de la tendencia con el parámetro changepoint_prior_scale.\nAjuste de la estacionalidad con add_seasonality y seasonality_mode.\nUso de add_country_holidays para incluir efectos de vacaciones.\nUso de make_future_dataframe para generar fechas futuras para pronósticos.\nInterpretación de los componentes del modelo con plot_components."
  },
  {
    "objectID": "data_science_ingemat/modulo2/proyecto2.html#rúbrica-del-proyecto-de-series-de-tiempo-con-prophet",
    "href": "data_science_ingemat/modulo2/proyecto2.html#rúbrica-del-proyecto-de-series-de-tiempo-con-prophet",
    "title": "Proyecto",
    "section": "Rúbrica del Proyecto de Series de Tiempo con Prophet",
    "text": "Rúbrica del Proyecto de Series de Tiempo con Prophet\n\n\n\n\n\n\n\n\n\n\nPuntos de Evaluación\nNivel 1\nNivel 2\nNivel 3\nNivel 4\n\n\n\n\nCalidad del análisis exploratorio de datos y las conclusiones extraídas\nEl análisis exploratorio de datos es inexistente o muy limitado, sin conclusiones significativas.\nEl análisis exploratorio de datos es básico, con algunas conclusiones pero falta profundidad o comprensión completa de los datos.\nEl análisis exploratorio de datos es sólido, con conclusiones bien razonadas y una buena comprensión de los datos.\nEl análisis exploratorio de datos es excepcionalmente detallado y perspicaz, con conclusiones profundas y una comprensión completa de los datos.\n\n\nCorrecta aplicación de Prophet y uso de sus funcionalidades\nProphet no se utiliza correctamente o no se utiliza en absoluto.\nProphet se utiliza de manera básica, pero no se explotan todas sus funcionalidades.\nProphet se utiliza correctamente y se explotan la mayoría de sus funcionalidades.\nProphet se utiliza de manera experta, aprovechando todas sus funcionalidades para mejorar el modelo.\n\n\nPrecisión de los pronósticos\nLos pronósticos son inexactos y el modelo no se ajusta bien a los datos.\nLos pronósticos son moderadamente precisos, pero el modelo podría mejorarse.\nLos pronósticos son precisos y el modelo se ajusta bien a los datos.\nLos pronósticos son extremadamente precisos y el modelo se ajusta excepcionalmente bien a los datos.\n\n\nInterpretación de los componentes del modelo\nNo se realiza ninguna interpretación de los componentes del modelo.\nSe realiza una interpretación básica de algunos componentes del modelo.\nSe realiza una interpretación sólida de la mayoría de los componentes del modelo.\nSe realiza una interpretación detallada y perspicaz de todos los componentes del modelo.\n\n\nCalidad de la presentación de sus resultados\nLos resultados no se presentan de manera clara o comprensible.\nLos resultados se presentan de manera básica, pero podrían mejorarse para una mayor claridad o comprensión.\nLos resultados se presentan de manera clara y comprensible.\nLos resultados se presentan de manera excepcionalmente clara, detallada y comprensible."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html",
    "href": "data_science_ingemat/modulo2/estacionaria.html",
    "title": "Tema 2: Estacionariedad",
    "section": "",
    "text": "Una serie de tiempo es considerada estacionaria si cumple con las siguientes propiedades estadísticas a lo largo del tiempo:\n\nTiene una media constante.\nTiene una varianza constante.\nLa covarianza entre los dos periodos (por ejemplo, t y t+m) depende solo de la diferencia m y no del tiempo t.\n\nFormalmente, una serie de tiempo {Xt} se considera estrictamente estacionaria si la distribución conjunta de (Xt1, Xt2, …, Xtk) es la misma que la de (Xt1+h, Xt2+h, …, Xtk+h) para cualquier elección de los tiempos t1, t2, …, tk y para cada desplazamiento h.\n\n\nExisten varias técnicas para determinar si una serie de tiempo es estacionaria. Algunas de las más populares incluyen:\n\nPrueba de Dickey-Fuller aumentada (ADF): Esta prueba hipotetiza que una serie de tiempo es no estacionaria (tiene alguna forma de raíz unitaria). Un resultado de prueba que rechaza esta hipótesis indica que la serie es estacionaria.\nPrueba de KPSS (Kwiatkowski-Phillips-Schmidt-Shin): A diferencia de la prueba ADF, la prueba KPSS hipotetiza que una serie de tiempo es estacionaria. Un resultado de prueba que rechaza esta hipótesis indica que la serie no es estacionaria.\n\n\n\n\nLa prueba de Dickey-Fuller Aumentada es una prueba de raíz unitaria en la presencia de estructura de serie autocorrelacionada. Para una serie de tiempo \\(y_t\\), la versión básica de la prueba de Dickey-Fuller considera la siguiente regresión de primer orden:\n\\(Δy_t = α + βt + γy_{t-1} + ε_t\\)\nLa hipótesis nula es que \\(γ = 0\\) (la serie tiene una raíz unitaria), mientras que la alternativa es \\(γ &lt; 0\\) (la serie es estacionaria). Para la versión aumentada de la prueba, se agregan términos de rezago de la serie diferenciada a la derecha de la ecuación de regresión para eliminar la autocorrelación en los errores (\\(ε_t\\)):\n\\(Δy_t = α + βt + γy_{t-1} + δ1Δy_{t-1} + … + δ_{p-1}Δy_{t-p+1} + ε_t\\)\n\n\n\nLa prueba KPSS es una prueba de hipótesis para probar la estacionariedad de una serie de tiempo (hipótesis nula) contra la presencia de una raíz unitaria (hipótesis alternativa).\nPara una serie de tiempo \\(y_t\\), la prueba KPSS considera la siguiente ecuación de regresión:\n\\(y_t = α + βt + St + ε_t\\)\ndonde \\(St\\) es una caminata aleatoria, que puede ser estocástica o determinística.\nLa hipótesis nula es que la serie es estacionaria (o trend-estacionaria), mientras que la alternativa es que la serie tiene una raíz unitaria.\n\n\n\nSerie Estacionaria: Las variaciones diarias de la temperatura (alrededor de la media) podrían ser consideradas como una serie estacionaria, ya que podríamos esperar que la variación media en la temperatura no cambie mucho de un día a otro.\nSerie No Estacionaria: El precio de una acción en el mercado es un ejemplo de una serie no estacionaria, ya que tiende a seguir una tendencia ascendente o descendente y no oscila alrededor de una constante.\n\nPara implementar pruebas de estacionariedad en Python, puedes usar la biblioteca statsmodels."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html#series-de-tiempo-estacionarias",
    "href": "data_science_ingemat/modulo2/estacionaria.html#series-de-tiempo-estacionarias",
    "title": "Tema 2: Estacionariedad",
    "section": "",
    "text": "Una serie de tiempo es considerada estacionaria si cumple con las siguientes propiedades estadísticas a lo largo del tiempo:\n\nTiene una media constante.\nTiene una varianza constante.\nLa covarianza entre los dos periodos (por ejemplo, t y t+m) depende solo de la diferencia m y no del tiempo t.\n\nFormalmente, una serie de tiempo {Xt} se considera estrictamente estacionaria si la distribución conjunta de (Xt1, Xt2, …, Xtk) es la misma que la de (Xt1+h, Xt2+h, …, Xtk+h) para cualquier elección de los tiempos t1, t2, …, tk y para cada desplazamiento h.\n\n\nExisten varias técnicas para determinar si una serie de tiempo es estacionaria. Algunas de las más populares incluyen:\n\nPrueba de Dickey-Fuller aumentada (ADF): Esta prueba hipotetiza que una serie de tiempo es no estacionaria (tiene alguna forma de raíz unitaria). Un resultado de prueba que rechaza esta hipótesis indica que la serie es estacionaria.\nPrueba de KPSS (Kwiatkowski-Phillips-Schmidt-Shin): A diferencia de la prueba ADF, la prueba KPSS hipotetiza que una serie de tiempo es estacionaria. Un resultado de prueba que rechaza esta hipótesis indica que la serie no es estacionaria.\n\n\n\n\nLa prueba de Dickey-Fuller Aumentada es una prueba de raíz unitaria en la presencia de estructura de serie autocorrelacionada. Para una serie de tiempo \\(y_t\\), la versión básica de la prueba de Dickey-Fuller considera la siguiente regresión de primer orden:\n\\(Δy_t = α + βt + γy_{t-1} + ε_t\\)\nLa hipótesis nula es que \\(γ = 0\\) (la serie tiene una raíz unitaria), mientras que la alternativa es \\(γ &lt; 0\\) (la serie es estacionaria). Para la versión aumentada de la prueba, se agregan términos de rezago de la serie diferenciada a la derecha de la ecuación de regresión para eliminar la autocorrelación en los errores (\\(ε_t\\)):\n\\(Δy_t = α + βt + γy_{t-1} + δ1Δy_{t-1} + … + δ_{p-1}Δy_{t-p+1} + ε_t\\)\n\n\n\nLa prueba KPSS es una prueba de hipótesis para probar la estacionariedad de una serie de tiempo (hipótesis nula) contra la presencia de una raíz unitaria (hipótesis alternativa).\nPara una serie de tiempo \\(y_t\\), la prueba KPSS considera la siguiente ecuación de regresión:\n\\(y_t = α + βt + St + ε_t\\)\ndonde \\(St\\) es una caminata aleatoria, que puede ser estocástica o determinística.\nLa hipótesis nula es que la serie es estacionaria (o trend-estacionaria), mientras que la alternativa es que la serie tiene una raíz unitaria.\n\n\n\nSerie Estacionaria: Las variaciones diarias de la temperatura (alrededor de la media) podrían ser consideradas como una serie estacionaria, ya que podríamos esperar que la variación media en la temperatura no cambie mucho de un día a otro.\nSerie No Estacionaria: El precio de una acción en el mercado es un ejemplo de una serie no estacionaria, ya que tiende a seguir una tendencia ascendente o descendente y no oscila alrededor de una constante.\n\nPara implementar pruebas de estacionariedad en Python, puedes usar la biblioteca statsmodels."
  },
  {
    "objectID": "data_science_ingemat/modulo2/estacionaria.html#tarea-1-jupyter-notebook",
    "href": "data_science_ingemat/modulo2/estacionaria.html#tarea-1-jupyter-notebook",
    "title": "Tema 2: Estacionariedad",
    "section": "Tarea 1: Jupyter Notebook",
    "text": "Tarea 1: Jupyter Notebook\nAhora debes poner manos a la obra y completar el siguiente notebook: Notebook 1 de Series de Tiempo"
  }
]